\documentclass[english,letterpaper,12pt]{report}
%\usepackage[margin=1in]{geometry}

% Encoding, fonts, and language (fold)
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2212}{\textminus}
\usepackage[T1]{fontenc}
\usepackage[pdftex,
            pdfauthor={Max Veit},
            pdftitle={Simulation of Genetic Regulatory Networks},
            pdfkeywords={stochastic simulation genetic regulatory networks epigenetics}]{hyperref}
\usepackage{fouriernc}
\usepackage{tgschola}
\usepackage{babel}
% (end)

% Mathematics and symbols (fold)
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{siunitx}
% (end)

% Figures (fold)
\usepackage{graphicx}
\usepackage{float}
\usepackage[font={small,it}]{caption}
\usepackage{subcaption}
\usepackage{placeins}
% (end)

% Gnuplot vector images (fold)
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}
% (end)

% Source code listings (fold)
\usepackage{listings}
\usepackage{algorithmic}
% (end)

% Text-level formatting (fold)
\usepackage{color}
\usepackage{setspace}
\usepackage{multicol}
\frenchspacing
\usepackage[square,numbers]{natbib}
\sisetup{per-mode=symbol-or-fraction}
%\numberwithin{equation}{section}
% (end)

% Convenience / typographical consistency
\newcommand{\defkeywd}[1]{\textbf{#1}}
\usepackage[enable]{easy-todo}
% Deprecated - don't play nice with latex-suite autocomplete
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}

% Custom math symbols, commands
\newcommand{\tenexp}[1]{\times10^{#1}}
\newcommand{\dee}{\;\mathrm{d}}
\let\oldvec\vec
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\evec}[1]{\ensuremath{\vec{e}_{#1}}} % standard basis vector
\newcommand{\norm}[2]{\ensuremath{\|#1\|_{#2}}}
\newcommand{\bignorm}[2]{\ensuremath{\left\|#1\right\|_{#2}}}
\newcommand{\infnorm}[1]{\ensuremath{\|#1\|_\infty}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator{\Prob}{P}
% Physics Domain-Specific
\newcommand{\kB}{\ensuremath{k_\mathrm{B}}}
% Document-specific
\newcommand{\delaytime}{\ensuremath{\tau}}

% Headers and Footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Max Veit\\University of Minnesota}
\rhead{Simulation of Genetic\\Regulatory Networks}
\chead[]{}
\cfoot{\thepage}
\setlength{\headheight}{27.7pt}

\begin{document}
\title{Stochastic Simulation of Genetic Regulatory Networks with Delayed Reactions}
\author{Max Veit}
\date{9 May 2014}

% ----- General Writing TODO notes ----- %
% Fill in all references as soon as possible
% Ensure tense consistency
% Bring everything up to AIP's style-guide standards
% See where I can change sentences to active voice
% -- Replace occurrences of "one" with the less-awkward royal "we"
% Watch for long prepositional phrases

%TODO Format the title page according to UHP requirements (including acknowledgements, non-technical summary, etc.
\maketitle

% May not be necessary, but nice to have for now.
\tableofcontents

\begin{doublespacing}

\chapter{Introduction} % (fold)
\label{sec:introduction}

%TODO Background references look a little thin - maybe add some more from browsing around?
Recent research in biological physics~\cite{ecoli-decision} indicates that the behavior and internal workings of a living cell are much more rich and complex than the bare instructions coded into its DNA would suggest. For example, an individual section of DNA can be turned off when a repressor protein binds to the beginning of that section. Such mechanisms provide cells a way to change the expression of their DNA, i.e. to control which proteins are produced from their genes and in what amounts, an ability known as (genetic) transcriptional regulation. This ability allows genetically identical cells to adapt their behavior to different environments or to differentiate into different types of cells as in the development of a multicellular organism. Developing models for the selective expression of genes is key to understanding how cells perform their daily functions and respond to their enviroments.

Another interesting feature of transcriptional regulation is the presence of feedback. Even the simplest cells (such as \textit{E. Coli}, a commonly-studied model organism in biological physics) have hundreds of individually switchable sets of  genes \cite{ecoli-operons}.  This complexity allows genetic switches to interact in interesting and useful ways, forming a genetic regulatory network. An example would be two regions of DNA that each code for proteins that suppress the other region, although much more complex feedback mechanisms are found in real cells as well as synthesized in laboratories. \todo{more examples besides those below?} Such mechanisms have the potential to be harnessed as a form of biological circuitry - computation done with chemical reactions instead of electricity~\cite{bio-circuits}. Researchers have already synthesized basic components, such as oscillators~\cite{synth-osc}. Biological circuitry promises to deliver a level of control over cells that would allow bacteria to be harnessed for producing chemicals, delivering drugs, or even replacing damaged human cells \todo{examples, refs}. Advancement of this field, however, depends on a better understanding of genetic networks and their behavior.

Many diverse strategies have been employed to model genetic regulatory networks~\cite{review-in-numero}\cite{bistable-modeling}\cite{gillespie-ssa}\cite{langevin-limit}. Many such models \todo{examples} approximate genetic regulation as a deterministic process. However, in very small systems (such as cells) where the effects of the finiteness of molecular populations become apparent, thermal fluctuations would be expected in inject a relatively large amount of stochasticity into the process. Recent experiments and theoretical considerations~\cite{ecoli-decision}\cite{stoch-theories}\cite{stoch-single-cell} make it clear that this stochasticity has a major influence on the function of these networks. In particular, there is a large amount of variation in genetic expression between individual cells in genetically identical colonies.

One way of accounting for this stochasticity is a technique known as system size expansion, which treats the concentrations of chemical species as continuous quantities, without any restriction that they be positive \todo{cite langevin-limit publication, research notes}. However, these models break down in a fundamental way when considering very small numbers of molecules, where the inherent discreteness of molecular concentrations plays an important role. \todo{could mention unstable oscillations in delayed-degradation system...}

The method explored in this work is to directly simulate the sequence of chemical reactions occurring in the cell using a Monte Carlo algorithm that naturally accounts for the stochasticity in discreteness present in small systems. The algorithm is known as the Gillespie stochastic simulation algorithm (SSA) and has seen use before in the context of genetic networks~\cite{we-chemkin}\cite{stoch-sys-bio}. The aim of this work is to extend the SSA to make it more practical for simulating real-world (natural or synthetic) genetic regulatory networks in order to analyze their behavior.

% section Introduction (end)

\section{Stochastic Chemical Kinetics} % (fold)
\label{sec:chemkin}

In order to study the behavior of genetic networks on a molecular level, they are usually modeled as chemical systems evolving under sets of coupled chemical reactions. The reactions represent actions such as the binding of repressors to DNA sites, protein production, and protein degradation. This representation allows genetic networks to be studied from the perspective of chemical kinetics, which seeks to understand the time evolution of the concentrations of the reactants in a system. In the case of genetic networks, the reactants are the proteins that characterize the dynamics of the genetic network. The \defkeywd{state} of a chemical system refers to the set of concentrations of all the reactants -- in this case, all the proteins, mRNA, and other biological molecules one is interested in -- involved in the system. As a chemical system evolves in time, it moves through the \defkeywd{state space} whose coordinates correspond to each of the individual concentrations.

One approach to modeling chemical systems is to go to the continuum limit, i.e. a very large (macroscopic) system where the effects of discreteness and thermal fluctuations are negligible. Specifically, this limit assumes that the smallest change possible in the concentration of any reactant $X$ (the difference caused by adding or removing one molecule of $X$) is negligibly small relative to its average concentration $\bar{x}$. An additional assumption is that the thermal fluctuations in the concentration are also negligible compared to the average, so every concentration can be treated as a \emph{deterministic} quantity. This second assumption is made by the formalism of reaction-rate equations (RRE), which specifies how to solve for the concentrations using a set of deterministic differential equations. 

Another technique, known as system-size expansion, makes the assumption of continuous concentrations but not of negligible fluctuations. The zeroth order of this expansion is equivalent to RRE. The first order is known as the Langevin limit; in this limit, it is possible to describe the extent of the thermal fluctuations from the RRE concentrations. However, neither theory is an exact description of very small chemical systems where discreteness plays an important role. \todo{Need ref on system size expn./Langevin limit}

%TODO Bit of a weak intro for stochastic chemkin - see if there's a more interesting way to bring it in
The theory of stochastic chemical kinetics explicitly treats the discreteness and stochasticity present in these systems. It avoids solving for a deterministic trajectory describing how the reactants evolve in time, as the RRE method does. Instead, it attempts to find the \emph{probability} that the chemical system will be be in a given state at a given time. It does this by employing the fundamental assumption that the probability that a given reaction $R_j$ will occur within the system volume in the next infinitesimal time interval of length $\dee t$ depends only on the current state $\vec{x}$. This probability is written as $a_j(\vec{x}) \dee t$, where the function $a_j(\vec{x})$ is known as the \defkeywd{propensity function} \cite{gillespie-ssa}. In other words, the fundamental assumption of chemical kinetics is that the chemical system can be represented as a (continuous-time) Markov chain.

Using the above assumption, it is possible to derive a differential equation that describes the time evolution of the probability distribution of the system. This probability distribution is written $\Prob(\vec{x}, t | \vec{x}_0, t_0)$, which means ``the probability, given that the system started in the state $\vec{x}_0$ at time $t_0$, that the system will be in the state $\vec{x}$ at some later time $t$.'' The equation is called the chemical master equation, and in the form given in \cite{gillespie-ssa}, it reads:
\begin{equation}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t | \vec{x}_0, t_0) = \sum_{j=1}^N \left( a_j (\vec{x} - \vec{s}_j) \Prob(\vec{x} - \vec{s}_j, t | \vec{x}_0, t_0) - a_j(\vec{x}) \Prob(\vec{x}, t | \vec{x}_0, t_0) \right)
    \label{eq:master-eqn-gillespie}
\end{equation}
The sum runs over all $N$ reaction pathways in the chemical system. The vector $\vec{s}_j$, known as the state-change vector of reaction $j$, indicates the effect of the reaction $j$ on the state of the chemical system: reaction $j$ takes the state instantaneously from $\vec{x}$ to $\vec{x} + \vec{s}_j$.

The first term under the sum can be seen as a source term; it represents the influx of probability into state $\vec{x}$ caused by the reaction $R_j$ taking the state $\vec{x} - \vec{s}_j$ to the state $\vec{x}$. Similarly, the second term can be seen as a drain term, representing the state change from $\vec{x}$ to $\vec{x} + \vec{s}_j$.

In order to simplify the above expression, we can average over all possible initial states $\vec{s}_0$ at time $t_0$ and write instead $P(\vec{x}, t)$. Additionally, we can write the equation more compactly by introducing the shift operator $\hat{T}^{\vec{s}_j}$, where $\hat{T}^{\vec{s}_j} P(\vec{x}, t) = P(\vec{x} + \vec{s}_j, t)$. The result is the more convenient form:
\begin{equation}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t) = \sum_{j=1}^N a_j(\vec{x})(\hat{T}^{-\vec{s}_j} - 1)\Prob(\vec{x}, t)
    \label{eq:master-eqn}
\end{equation}

The above definition of propensity and the resulting Master equation rely on several simplifying assumptions about the system. The most severe of these is that the chemical system is assumed to be a homogeneous (well-stirred) ideal gas or dilute solution. \todo{may need reference that details assumptions, since Gillespie (2007) doesn't go into much detail} While these assumptions begin to break down in living cells (see Section~\ref{sub:diffusion-crowded}), the utility and ease of implementation of algorithms to solve the chemical master equation have led to wide use of this theory in modeling the biochemistry of cells~\cite{we-chemkin}\cite{stoch-sys-bio}.

\subsection{Propensity Functions} % (fold)
\label{sub:propensities}

The propensity functions defined above have different forms depending on the order of the associated reactions, i.e. whether they are production (zero-order), unimolecular, bimolecular, or higher-order reactions \cite{gillespie-ssa}. The order of the reaction refers to the number of separate molecules that constitute the inputs, or reactants. A production reaction, i.e. a reaction with no input reactants, can simply be modeled by using a constant propensity $a_j^{(0)} = c_j^{(0)}$. 

For a unimolecular reaction, e.g. a reaction $R_j$ taking one molecule of $X_1$ to some products, the probability that an isolated molecule of $X_1$ will undergo the reaction in the next infinitesimal time interval $\dee t$ can be assumed to be a constant $c_j^{(1)} \dee t$. The probability of \emph{any} reaction $R_j$ occurring within the system volume in the time interval $\dee t$ is thus proportional to the number of molecules of $X_1$ in the system, so $a_j^{(1)}(\vec{x}) = c_j^{(1)} x_1$.

For a bimolecular reaction, the propensity is the probability (per unit time) that any pair of reactant molecules will collide within the system volume, multiplied by the probability that such a collision will actually result in a reaction. One could therefore assume the probability per unit time of any pair of molecules reacting is a constant $c_j^{(2)}$, implying the propensity function is proportional to the number of pairs of reactants in the system volume. If the two reactant molecules are of different species $X_1$ and $X_2$, the propensity $a_j^{(2)}(\vec{x}) = c_j^{(2)} \cdot x_1 x_2$. If they are of the same species $X_1$, $a_j^{(2)}(\vec{x}) = c_j^{(2)} \cdot \frac{1}{2}x_1(x_1 - 1)$ (which comes from $c_j^{(2)}\binom{x}{2}$, where $\binom{n}{k}$ denotes the binomial coefficient).

\todo{Discuss higher-order reactions? Gillespie says they don't technically exist, but some theoretical models use them.}

% subsection propensities (end)

\subsection{Delayed Reactions} % (fold)
\label{sub:delayed-reactions}

In order to reduce the computational cost and model complexity of applying the SSA to biological systems, an additional abstraction is adopted. Processes common in cell biology, such as DNA replication, protein production, and protein digestion, actually consist of sequences of thousands of individual chemical reactions like the binding of individual nucleotides to a developing RNA strand. It would be tedious and expensive to simulate each individual step of the process. More importantly, it would be extremely wasteful to do so if one is only interested in the high-level dynamics of proteins and genes, not the details of the constituent process.

One can avoid simulating each step of a complex biological process by modeling the entire process as a single reaction. For example, a protein-production reaction (which itself consists of many complex multi-step processes such as RNA transcription and protein folding) can be abstracted as a single reaction that produces a protein from nothing. However, this reaction cannot be said to occur instantaneously (as with simple chemical reactions), as the entire process it represents requires a certain duration to complete. Experimental evidence indicates that the duration of biologically important reactions such as protein production can be on the order of minutes to hours. Since this duration is long compared to the timescales of other cellular processes, it is important to account for it in the modeling of genetic networks \cite{delay-oscillations}.

To account for this duration, one can associate a \defkeywd{delay} with the reaction to represent the time the underlying process needs to complete. In effect, the propensity function for the delayed reaction depends not on the current state, but on the history of the chemical system. More precisely, if reaction $j$ is delayed with a time $\delaytime_j$, its propensity function is written $a_j \left(\vec{x}(t - \delaytime_j) \right)$. For simplicity, $\delaytime_j$ is assumed to be a number. In general, however, the delay may be better characterized by a probability distribution.

In the case where some of the reactions in a system are delayed, Equation~\eqref{eq:master-eqn} can be written:
\begin{align}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t) &= \sum_{j=P+1}^N a_j(\vec{x})(\hat{T}^{-\vec{s}_j} - 1)\Prob(\vec{x}, t) \\
                                                  &+ \sum_{j=1}^P \sum_{\vec{x}'} H(\vec{x}) a_j(\vec{x}') (\hat{T}^{-\vec{s}_j} - 1) \Prob_2(\vec{x}, t;\: \vec{x}', t - \delaytime_j)
    \label{eq:master-eqn-delay}
\end{align}
where $P$ of the $N$ total reactions are delayed and each delay $\delaytime_j$ is assumed to be a single number. The second sum term involves an inner sum over all states $\vec{x}'$ in the state space; the joint probability distribution $\Prob_2(\vec{x}, t;\: \vec{x}', t - \delaytime_j)$ gives the probability of the system visiting state $\vec{x}'$ at time $t - \delaytime_j$ \emph{and} reaching state $\vec{x}$ at time $t$. The inner sum thus covers all possible paths the system could take to reach the current state $\vec{x}$ over the duration of the delay. The multidimensional step function $H(\vec{x})$ is included to block unphysical reactions, i.e. those that take any single concentration below zero (the value of $H(\vec{x})$ is one if all the $x_i$ are positive, zero otherwise). 

Finally, introducing delayed reactions has an important consequence: Since delayed reactions depend on the history of the chemical system, models incorporating them are non-Markovian. This fact severely limits the analytical tools one can apply to analyze the stochastic behavior of systems with delayed reactions. It also has important consequences in the numerical simulation of delayed systems. These consequences are explored in Section~\ref{sub:we-delays}.

% subsection delayed-reactions (end)

% section chemkin (end)

\chapter{Methodology} % (fold)
\label{sec:methodology}

\section{Gillespie Stochastic Simulation Algorithm} % (fold)
\label{sub:gillespie-ssa}

One of the most well-known and widely used \todo{find examples} algorithms for simulating stochastic chemical systems is the stochastic simulation algorithm (SSA), which was introduced in 1976 by Daniel Gillespie~\cite{gillespie-1976}. As a Monte Carlo technique, it does not attempt to solve the chemical master equation \eqref{eq:master-eqn} explicitly. Rather, the approach taken by the SSA is to generate a \defkeywd{trajectory}, which is the path $\vec{x}(t)$ that one possible \emph{instance}, or example, of the chemical system might take through the state space given some initial conditions $\vec{x}(t_0) = \vec{x}_0$. Unlike the deterministic trajectories found using RRE, these trajectories are generated probabilistically. In effect, the SSA simulates the time evolution of a single instance of the chemical system with some arbitrary initial conditions.

The trajectories generated by the SSA can be seen as samples of the underlying probability distribution $\Prob(\vec{x}, t | \vec{x}_0, t_0)$ that describes the chemical system. In principle, one can generate a very large number of samples (trajectories) in order to estimate a distribution that converges to the true one.

The SSA generates trajectories (with the initial conditions $\vec{x}(t_0) = \vec{x}_0$) by repeating the following steps for each iteration $n$ \cite{gillespie-ssa}:
\begin{enumerate}
    \item Compute the propensities $a_j(\vec{x}_n)$ for all reactions $j$ and their sum $a_\text{tot}(\vec{x}_n)$ (see Section~\ref{sub:propensities}).
    \item Choose the next reaction type and the waiting time until that reaction from the following probability distributions:
    \begin{itemize}
        \item Waiting time: $\Prob(t_w)\dee t_w = a_\text{tot}(\vec{x}_n) \exp(-a_\text{tot}(\vec{x}_n) t_w) \dee t_w$
        \item Reaction type: $\Prob(j) = a_j(\vec{x}_n) / a_\text{tot}(\vec{x}_n)$
    \end{itemize}
    \item Execute the reaction, i.e. update the current state and time:
    \begin{itemize}
        \item $t_{n+1} = t_n + t_w$
        \item $\vec{x}_{n+1} = \vec{x}_n + \vec{s}_a$, where $\vec{s}_a$ is the state-change vector for the reaction chosen above
    \end{itemize}
\end{enumerate}
The iteration is continued typically until the time $t$ reaches (exceeds) a predefined stop time. Since a chemical system remains in its current state until another reaction occurs, the sequence $\left(\vec{x_n}, t_n\right)_{n=1}^N$ can be interpreted as the continuous-time trajectory $\vec{x}(t)$ of the instance.

\todo{Explain physical origin of probability distributions}

% subsection gillespie-ssa (end)

\section{Extension to Non-Markovian Dynamics} % (fold)
\label{sub:non-markovian}

The classic SSA was designed only for explicitly Markovian chemical systems. However, one could imagine extending the algorithm to include delayed reactions. One such extension is proposed by \cite{delay-oscillations}. The modified algorithm, upon selecting a delayed reaction in Step~2 (see Section~\ref{sub:gillespie-ssa}), schedules the reaction to fire at a later time (that is, Step~3 is postponed by a time $\tau_a$). \todo{Mention what's wrong with this approach?}

In this work, a method that more closely follows the analytical modeling of delayed reactions via Equation~\eqref{eq:master-eqn-delay} is chosen. The modification is made in the propensity calculation of Step~1: To calculate the propensity of a reaction $R_j$ with delay $\delaytime_j$, one simply uses the state from $\delaytime_j$ time units ago. Thus, with a trajectory at time $t$, the propensity would be computed as $a_j = a_j\left(\vec{x}(t - \delaytime_j)\right)$. This is consistent with the delayed-reaction formulation of the master equation, Equation~\eqref{eq:master-eqn-delay}.

One undesirable feature is introduced by including delayed reactions using either method: Since the delayed reactions lag behind the current state, it is possible for individual concentrations to go below zero, an obviously unphysical result. In the non-delayed SSA, any reaction that could decrease the concentration $x_i$ has a propensity proportional to that concentration (to some power). Therefore, when $x_i$ goes to zero, any reaction that could decrease $x_i$ has a zero propensity and is blocked from running. A delayed reaction, however, does not ``notice'' zero concentrations in real time so it is not blocked in time to avoid decreasing concentrations from zero\footnote{Some analytical models of delayed systems have this problem owing to the difficulty of analytically constraining concentrations to be nonnegative. The result is often nonsensical behavior, such as oscillations with exponentially increasing amplitude.}.

One solution is to manually impose the constraint $x_i \geq 0\; \forall i$, which is much easier to do within the SSA than in analytical models. The method used in this work is to manually block the offending reactions: In Step~2 of the SSA, if a reaction is selected that would make any individual concentration go negative, the reaction is discarded and another is selected. (This amounts to manually setting that reaction's propensity to zero.) \todo{May change code to actually set propensity to zero - stay tuned}

% subsection non-markovian (end)

\section{Weighted-Ensemble Resampling} % (fold)
\label{sub:we-resampling-intro}

The most straightforward way to obtain a probability distribution using the SSA is to run an ensemble of trajectories in parallel, then estimate the probability density $P(\vec{x}, t | \vec{x}_0, t_0)$ from those samples\footnote{If the probability distribution is known to be steady-state, i.e. independent of time, then one can simply average a single trajectory over a long period of time to obtain $P(\vec{x})$. However, in systems where time-independence is not known \textit{a priori}, the ensemble method must be used.}.

The main issue with this method is that it generally samples state space unevenly. Trajectories in the ensemble tend to congregate, by construction, in the most probable regions of state space while less probable regions (the ``tails'' of the distribution) are more rarely visited. The result is that the accuracy of the sample of the probability distribution obtained using this simple ensemble increases with the probability density at the same location. This effect makes this method extremely inefficient if one is most interested in the least probable regions, as is the case in many problems in chemistry and biology (e.g. computing transition rates).
\todo{Examples relating to genetic networks? Bistable/Schlögl systems, maybe?}
In many cases, \todo{Illustrative example?} one would need to use a prohibitively large number of trajectories to adequately sample the improbable transition regions.
\todo{Include illustration if appropriate}

To more uniformly and efficiently sample the phase space, one can use a method known (in the context of molecular dynamics and stochastic simulation) as the \defkeywd{weighted ensemble} (WE) method~\cite{we-orig}, a type of importance sampling. The basic strategy is to periodically redistribute the samples in a way that does not change the final estimate of the probability distribution $P(\vec{x}, t | \vec{x}_0, t_0)$. This is achieved by assigning each trajectory a \defkeywd{weight} $w_k$ such that the sum of all weights is always 1. In effect, the method biases the underlying distribution so that rare events are sampled as frequently as the common ones while keeping track of weights in order to obtain a sample of the original, \emph{unbiased} probability distribution.

The WE method has been applied to stochastic chemical kinetics before; see~\cite{we-chemkin}. The method used in this work generally follows the version presented in the paper. The overall procedure is as follows: First, choose an initial ensemble of $P$ trajectories. Then, repeat the following steps as many times as desired:
\begin{enumerate}
    \item Perform a resampling step to equalize the distribution of trajectories over state space.
    \item Run all the trajectories independently for a time $\tau_p$.
\end{enumerate}
In this case, running a trajectory means applying the SSA to evolve the trajectory forward in time. However, the generality of the weighted-ensemble method permits the use of other time evolution algorithms for the second step. In the more commonly encountered applications of the weighted-ensemble method, a molecular dynamics algorithm is used to evolve independent trajectories through phase space.

The resampling step is implemented by partitioning the state space into discrete \defkeywd{bins}. To equalize the distribution of trajectories over state space, two basic operations can be used: To increase the number of trajectories in a bin, a trajectory $T_a$ can be replicated (or ``split'') to obtain $M$ copies. Each of the copies receives a weight $w_a/M$; in this way, the total weight within a bin is conserved. To decrease the number of trajectories, a trajectory $T_b$ can be deleted (or ``killed'') and its weight assigned to another trajectory $T_c$ in the same bin, i.e. $T_c$ receives a new weight $w_b + w_c$. Again, the total weight within each bin is conserved. This conservation of weight is the key feature that ensures the statistical correctness of the resampling method.

In this work, I adopt a binning strategy that spaces bin boundaries uniformly over some range $[x_{i,\text{min}}, x_{i,\text{max}}]$ for each coordinate $x_i$; coordinates outside this range are grouped into the closest in-bounds bin. These bins are statically defined, so the estimate of the probability distribution is taken to be a histogram over the bins. This is by no means the only strategy possible; \cite{we-exact} gives more examples of binning strategies to show how general the weighted-ensemble method is.

% subsection we-resampling-intro (end)

\section{Weighted-Ensemble SSA} % (fold)
\label{sub:wessa}

Some important conceptual issues arise when applying the weighted-ensemble method to chemical-kinetics trajectories evolved using the SSA. The WE method is only guaranteed to be statistically exact if the resampling step occurs at the same time for all the trajectories in the ensemble. However, the SSA chooses non-uniform timesteps, so it would seem impossible to ``pause'' all the trajectories at the same time $t_\text{sync}$. 

Figure~\ref{fig:wessa-pause} illustrates the problem. The SSA only generates a discrete set of steps $(\vec{x}_n, t_n)$. Thus, the closest one can to come to pausing a trajectory at $t_\text{sync}$ is to stop each trajectory once it chooses a reaction time that is after $t_\text{sync}$, but before the trajectory actually updates its state. However, this procedure results in an ensemble of trajectories each at different times, which means that it may not be correct to apply resampling to this ensemble.

\begin{figure}[ht]
    \centering
    \def\svgwidth{0.65\textwidth}
    \input{figures/wessa-timestep.pdf_tex}
    \caption{The SSA takes non-uniform timesteps, so it is not possible to pause each trajectory exactly at time $t_\text{sync}$. Instead, one must pause it at the last reaction before $t_\text{sync}$, resulting in an ensemble of trajectories each at different times.}
    \label{fig:wessa-pause}
\end{figure}

In the case of purely Markovian (non-delayed) dynamics, it is in fact perfectly valid to apply WE resampling to the set of paused trajectories described above. The reason is that the future probability distribution for a Markovian trajectory depends, by definition, only on its current state. This means the distribution of waiting times until the next reaction on a trajectory is the memoryless exponential distribution. If a trajectory determines its next reaction to occur after the time $t_\text{sync}$, it is thus possible to update the trajectory's time to $t_\text{sync}$ and sample the next reaction from there without biasing the overall measured probability distribution.

Therefore, it is valid to pause a Markovian trajectory at an arbitrary time $t_\text{sync}$. This is statistically equivalent to leaving the trajectory at the state of its last reaction before $t_\text{sync}$. One can thus run WE resampling on trajectories paused this way without biasing the estimate of the underlying probability distribution.

% subsection wessa (end)

\section{Weighted Ensemble With Delays} % (fold)
\label{sub:we-delays}

In systems with non-Markovian dynamics, i.e. delayed reactions, a different resolution is necessary in order to be able to apply the weighted-ensemble method. Since the propensities for delayed reactions depend on a trajectory's history, they can change in between reactions. The distribution of waiting times for a given reaction is thus no longer memoryless.

To circumvent this problem, this work uses the following implementation of trajectory pausing: If a trajectory is to be paused at a time $t$ it executes the SSA until a reaction is generated that would fire later than $t$. Instead of executing that reaction, the trajectory saves it and becomes paused. Upon resuming, the trajectory recalls and executes the saved reaction and continues the SSA from the time of that reaction.

However, it is still an open question whether it is correct to pause an SSA trajectory in between reactions. The more fundamenal question is whether it is valid to interpret the discrete sequence $(\vec{x}_n, t_n)$ generated by the SSA as a continuous-time trajectory. This question is not easy to address on a mathematically rigorous level. However, numerical evidence from systems with analytical descriptions indicates that WE resampling does not bias the numerical estimate of the probability distribution. This conclusion holds on systems that include delayed reactions.

% subsection we-delays (end)

% section Methodology (end)

\chapter{Results} % (fold)
\label{chp:results}

\section{Simple Production-Degradation System} % (fold)
\label{sec:simple-pd}

Accurate analytical distributions exist only for the simplest model systems. One such system is the production-degradation system, consisting of two reactions with the equations:
\begin{align}
    \begin{gathered}
        \varnothing \xrightarrow{A} X \\
        X \xrightarrow {B} \varnothing
    \end{gathered}
    \label{eq:prod-deg-rxn}
\end{align}
The parameters $A$ and $B$ are the propensity constants for each reaction. The parameter $A$ sets the scale of the system; it is often written as the product $\Omega A$ of the system's volume and a parameter of order one. Here, for simplicity, the system volume is absorbed into the constant $A$. Using this notation, the parameter $A$ plays a similar role to the volume $\Omega$.

%TODO Make distinction between population and concentration more clear
The deterministic RRE method predicts an equlibrium population of $n^\star = \frac{A}{B}$. In the Langevin limit (large $\Omega$, continuous concentration $x$), the steady-state probability distribution is predicted to be a Gaussian with standard deviation also equal to $\frac{A}{B}$. Explicitly,
\begin{equation}
    \Prob(n) = \sqrt{\frac{B}{2\pi A}}\exp\left( -\frac{B}{2 A} \left( n - \frac{A}{B} \right)^2  \right)
    \label{eq:prod-deg-dist}
\end{equation}

Alternatively, since the fluctuations in the population are expected to scale as $\sqrt{A}$, it makes sense to define the normalized fluctuation $\xi = A^{-1/2}(n - n^\star)$. In terms of $\xi$, the steady-state probability distribution is then
\begin{equation}
    \Prob(\xi) = \sqrt{\frac{B}{2\pi}} \exp(-B \xi^2)
    \label{eq:pd-dist-xi}
\end{equation}

\subsection{Weighted-Ensemble Verification} % (fold)
\label{sub:pd-we-verif}

\begin{figure}[t]
    % The "big" version
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-cmp.pgf}
        %\end{center}
        %\label{sfg:pdwe1-comp}
    %\end{subfigure}
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-devs.pgf}
        %\end{center}
        %\label{sfg:pdwe1-chi}
    %\end{subfigure}
    % More compact one-liner version
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a400-cmp-small.pgf}
            \end{center}
            \label{sfg:pdwe4-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a400-devs-small.pgf}
            \end{center}
            \label{sfg:pdwe4-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of the weighted-ensemble method with a straight non-reweighted ensemble on the simple production-degradation system with $A=400$ and $B=3$. The probability distributions were taken after $T=60$ (arbitrary) time units. Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:pdwe4}
\end{figure}

The system was simulated using the weighted-ensemble method with individual trajectories evolved using the unmodified SSA. The ensemble was evolved in a sequence of steps of duration $\tau_p$ up to a total time $T$. At the end of the run, the probability distribution was estimated as the sum of weights within each bin. As described in Section~\ref{sub:we-resampling-intro}, the division of the state space into a number $N_b$ of bins was done uniformly over the coordinate $n$. The boundaries of the bins were linearly spaced between values $n_\text{min}$ and $n_\text{max}$. Any trajectories with states outside this range were placed in the nearest in-bounds bin. The initial trajectories used to seed the WE method were assigned random initial states chosen from the uniform distribution on $[n_\text{min}, n_\text{max}]$.

Resampling was done at the beginning of each step, ensuring each bin had the target number of trajectories $N_T$. Note that it is not possible to increase the number of trajectories in empty bins. By resampling often enough (choosing $\tau_p$ sufficiently small) and using a large enough target number $N_T$, one can be reasonably certain that no bins will become empty within the concentration range one is interested in. Unless otherwise noted, the parameters used were $\tau_p = 0.01$ and $N_T = 10$; each ensemble was inspected at the end of the run to make sure all relevant bins were populated. The total time $T$ was chosen so that the samples would have equilibriated to the steady-state distribution by then. This equilibriation time was determined by inspecting the sequence of distributions generated by a sample run of the WE simulation.

For comparison, a similar simulation was done with the resampling turned off at each step; this is equivalent to running all the initial trajectories in parallel up until the time $T$. The number of trajectories used was equal to the number of bins multiplied by the target number $N_T$. The probability distribution was estimated simply as the number of trajectories within each bin (normalized by the total number of trajectories). 

A comparison between the resampled and non-resampled distributions with a reasonably large system volume is shown in Figure~\ref{fig:pdwe4}. 

\begin{figure}[t]
    % The "big" version
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-cmp.pgf}
        %\end{center}
        %\label{sfg:pdwe1-comp}
    %\end{subfigure}
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-devs.pgf}
        %\end{center}
        %\label{sfg:pdwe1-chi}
    %\end{subfigure}
    % More compact one-liner version
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a100-cmp-small.pgf}
            \end{center}
            \label{sfg:pdwe1-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a100-devs-small.pgf}
            \end{center}
            \label{sfg:pdwe1-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of weighted and non-reweighted ensembles against the analytical distribution for the simple production-degradation system with $A=100$ and $B=3$ ($T=60$, $\tau_p = 0.02$), showing discreteness effects. The discrete minimum $n=0$ corresponds to $\xi \approx -3.3$. Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:pdwe1}
\end{figure}

In order to compare the two distributions in a statistically meaningful way, it is useful to estimate the uncertainty of the value of any given bin. A na\"{i}ve estimate can be obtained by running many separate ensembles with the same parameters, then taking the standard error $s_i$ of the values for a given bin $i$ across all the resulting distributions. Reported here is the mean value of each bin across $N_\text{ens}$ independent ensembles, with the standard error on that mean estimated by $\frac{s_i}{\sqrt{N_\text{ens}}}$. The number of ensembles used was $N_\text{ens} = 40$ for all the results presented here.

\begin{figure}[ht]
    \begin{center}
        \input{../results/thesis/pdwe-a400-varcloseup.pgf}
    \end{center}
    \caption{Close-up of the probability distribution estimates of Figure~\ref{fig:pdwe4}, showing the variances in a low-probability area of the concentration space.}
    \label{fig:pdwe-varcloseup}
\end{figure}

We would expect to see the WE method improving our sampling over low-probability regions of the concentration space. Intuitively, if a bin has been visited by ten traectories, one can be much more certain about the reported value for that bin than if only one trajectory was found there. This improved sampling should manifest itself in a reduced variance in the values of the bins at the tails of the distribution. As Figure~\ref{fig:pdwe-varcloseup} shows, though, the na\"ive estimates of the bin variances are inconsistent with this expectation. Estimated variances for the resampled and non-resampled distributions are approximately the same even in the tail of the distribution. This characteristic is reflected in all the weighted-ensemble datasets presented in this work.

\todo{Show plot of bin populations, mention that statistics \emph{should} be better for WE}

This inconsistency of the variance estimates with expectations could indicate one of two things. First, the na\"ive estimate of bin variance could be failing to capture the improvement in sampling that the WE method offers. This seems unlikely, however, as the variances were estimated in a statistically correct manner. The second possible effect is that the target number $N_T$ of trajectories per bin is not large enough to afford a significant improvement in the variance of low-probability bins. Intuitively, however, one would expect a variance reduction even with the numbers used here ($N_T = 10$ trajectories versus one or two in some non-resampled bins). Perhaps it would be possible to get a better estimate of the variance of a bin by taking into account the number of trajectories, i.e. the number of samples, that were used to find its value.

In any case, none of the comparisons here shows a \emph{systematic} difference between the resampled and non-resampled distributions. Therefore, it can be concluded that WE resampling does not bias our estimate of the probability distribution for the simple production-degradation system.

This conclusion is exactly what one would expect, as this system has no delayed reactions and is therefore Markovian. As discussed in Section~\ref{sub:wessa}, WE resampling is guaranteed to be correct for Markovian systems. For the purposes of verifying the WE method, it is more interesting to study systems that incorporate delayed reactions.

% subsection pd-we-verif (end)

% subsection simple-pd (end)

\section{Delayed-Degradation System} % (fold)
\label{sec:delayed-deg}

One of the simplest possible systems that includes delayed reactions is the delayed-degradation model, defined by the reactions:
\begin{align}
    \begin{gathered}
        \varnothing \xrightarrow{A} X \\
        X \xrightarrow {B} \varnothing \\
        X \xRightarrow[(\tau)]{C} \varnothing
    \end{gathered}
    \label{eq:delayed-deg-rxns}
\end{align}
This is simply the production-degradation system with an additional reaction added; the notation for the third reaction means it is a delayed reaction with propensity constant $C$ (analogous to $B$) and delay $\tau$. As in the simple production-degradation model the parameter $A$ sets the scale of the system, absorbing the system volume $\Omega$.

The additional delayed reaction has a profound effect on the overall behavior of this system. For large enough values of $C$ the system undergoes a Hopf bifurcation and the concentration oscillates periodically between two values (the ``oscillatory mode''). However, for smaller values of $C$ the system behaves much like the simple production-degradation system, fluctuating about a stable mean value (the ``stable mode''). An analytical form for the probability distribution is only available for the stable mode. In the limit of short delay $\tau$, the distribution is \cite{delayed-deg-notes}
\begin{equation}
    P(\xi) = \sqrt{\frac{B + C}{2\pi(1 + C\tau)}}\exp\left( -\frac{B + C}{2(1 + C\tau)} \xi^2 \right)
    \label{eq:dd-analytic-dist}
\end{equation}
where $\xi$ is defined as in Section~\ref{sec:simple-pd}, only here the steady-state mean population is $n^\star = \frac{A}{B + C}$. The effect of the delay is to broaden the distribution compared to the simple production-degradation system.

\subsection{Weighted-Ensemble Verification} % (fold)
\label{sub:dd-we-verif}

The procedure used to assess the effect of resampling on the numerical probability distribution for this system is the same as the one of Section~\ref{sub:pd-we-verif}, with a few modifications. The first modification accounts for the fact that the delay introduces an effective phase to each trajectory (the effect is more obvious in the oscillatory mode). To ensure that the probability distributions were sampled uniformly over this phase, the starting times of the seed trajectories were randomized relative to each other. Specifically, the initial time of each trajectory, relative to the ensemble time, was drawn from a uniform distribution on $[0, \tau]$.

Second, the total time $T$ that each ensemble was run had to be chosen to avoid any effects due to the initial phase randomization. The value $T = 3\tau$ was deemed a safe choice for this purpose, as trajectories sample their histories only as far as $\tau$ time units earlier. By time $3\tau$, all trajectories would have been sampling active histories (that is, not sampling earlier than their starting times) for at least $\tau$ time units.

\begin{figure}[t]
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t0,1-a800-cmp-small.pgf}
            \end{center}
            \label{sfg:ddwe0-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t0,1-a800-devs-small.pgf}
            \end{center}
            \label{sfg:ddwe0-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of weighted and non-reweighted ensembles against the analytical distribution for the delayed-degradation system with $A=800$, $B=3$, $C=1$, and $\tau=0.1$ ($T=3$). Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:ddwe0}
\end{figure}

\begin{figure}[t]
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t1-a800-cmp-small.pgf}
            \end{center}
            \label{sfg:ddwe1-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t1-a800-devs-small.pgf}
            \end{center}
            \label{sfg:ddwe1-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of weighted and non-reweighted ensembles against the analytical distribution for the delayed-degradation system with $A=800$, $B=3$, $C=1$, and $\tau=1$ ($T=3$), showing deviation from the analytical variance derived for small $\tau$. Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:ddwe1}
\end{figure}

Based on this data, we can tentatively conclude that weighted-ensemble resampling does not bias our estimate of the steady-state probability distribution of the delayed-degradation system. This is a stronger conclusion than the one for the simple production-degradation system. This data provides strong evidence that, even in the presence of delayed reactions, the WE resampling method does not have a biasing effect. It remains to be seen whether this conclusion holds in general for more complex delayed systems, although this data does provide good reason to be optimistic.

% subsection dd-we-verif (end)

\subsection{Delayed Joint Probability Distribution} % (fold)
\label{sub:delayed-joint-dist}

A key problem in modeling the delayed-degradation system is to describe the delayed joint probability distribution $P_2(\vec{x}, t; \vec{x}', t - \tau)$ that appears in the delayed master equation (see Equation~\eqref{eq:master-eqn-delay}). In the specific case of the delayed-degradation system, this distribution can be written $P_2(n, t; m, t - \tau)$ in terms of integer populations $n$ and $m$ of the species $X$. One can also average Equation~\eqref{eq:master-eqn-delay} over $n(t)$, in which case the joint distribution turns into a \defkeywd{conditional average}:
\begin{equation}
    \langle m, t - \tau | n, t \rangle = \frac{\sum_m m P_2(n, t; m, t - \tau)}{\sum_m P_2(n, t; m, t - \tau) }
    \label{eq:cond-avg-def}
\end{equation}

One method for modeling the delayed joint distribution is used in \cite{delay-oscillations}, where events at time $t$ are assumed to be decorrelated from events at time $t - \tau$. This assumption implies $\langle m, t - \tau | n, t \rangle = \langle m, t - \tau \rangle$ -- in other words, that the conditional average of $m(t - \tau)$ is a constant that does not depend on $n(t)$. While this assumption may hold far away from the bifurcation point (in either direction), it could very well break down close to the bifurcation.

In order to test this assumption, it is possible to compute the delayed joint distribution based on a single trajectory generated by the SSA. Interpreting the sequence $(n_i, t_i)$ as a continuous-time trajectory $n(t)$, the probability $P_2(p, t; q, t - \tau)$ is proportional to the amount of time the system spends in the state $n(t) = p; n(t - \tau) = q$. More formally,
\begin{equation}
    P_2(p, t; q, t - \tau) = \frac{1}{t_1 - t_0} \int_{t_0}^{t_1} \delta_{p, n(t)} \delta_{q, n(t - \tau)} \dee t
    \label{eq:joint-pdist-calc}
\end{equation}
where $t_0$ is at least $\tau$ time units later than the trajectory's starting time; $\delta_{i,j}$ is the Kronecker delta symbol. The conditional average is then straightforward to calculate from Equation~\eqref{eq:cond-avg-def}.

\todo{Figure showing sequence of joint distributions sweeping through bifurcation, describe qualitative behavior (unimodal to bimodal)}

\todo{Show conditional average in stable mode}

\todo{Show conditional average at bifurcation, compare with Langevin-limit prediction}

\todo{Conditional average above bifurcation, showing saturation}

% subsection delayed-joint-dist (end)

% section delayed-deg (end)

\chapter{Conclusions and Future Work} % (fold)
\label{sec:conclusions}

The weighted-ensemble stochastic simulation algorithm is a useful tool for studying genetic networks with delays. The SSA explicitly treats the discreteness and stochasticity that become significant in small systems, allowing them to be described in a statistically exact manner. In addition, it is straightforward to extend the SSA to include delayed reactions. The WE resampling method ensures that the system's state space is sampled evenly, enabling the efficient simulation of realistic, complex genetic networks. Results from this work inidicate that the resampling maintains statistical accuracy, i.e. it does not bias the measured distribution, even in systems containing delayed reactions.

An important application of numerical simulation is to assess approximations or assumptions that analytical models make to study a system's behavior. In the specific case of the delayed-degradation system, one such assumption is that the behavior of the system decorrelates on the timescale $\tau$. However, the numerical results presented here do not support that assumption when the system is close to its bifurcation point.

% Future Work
The next step in this project is to model systems that are also realizable experimentally. A comparison between numerical predictions and experimental results would serve as an excellent check on the validity of the assumptions underlying the application of the SSA to genetic networks.

\section{Modeling Crowded Environments} % (fold)
\label{sub:diffusion-crowded}

The most important assumption underlying the SSA and the chemical master equation is that the chemical system is well-stirred (homogeneous) and behaves like an ideal gas. \todo{Reference, use same one from Section~\ref{sec:chemkin}}. Both of these assumptions can be expected to break down, to some extent, in living cells. First, cells are highly nonhomogeneous environments, with various internal membranes (especially, in eukaryotic cells, the nuclear membrane) hindering the free movement of molecules throughout the cell. Second, the high concentrations of bulky molecules such as proteins would also be expected to slow down the motion of molecules \todo{Reference}.

There are several possible ways to model these deviations from the ideal-gas assumption. One way would be to add a phenomenological retardation factor that decreases each reaction's propensity in the presence of high concentrations of other molecules. Another way would be to run the SSA on a spatial grid of cells coupled by a diffusion law.

Although it may seem improper to model a complex cellular environment as a simple ideal gas, this is still the dominant method for modeling genetic regulatory networks \todo{citation needed}. Comparison with experimental data should be the most useful guide in determining whether the ideal-gas assumption is in fact justified or whether one of the above strategies needs to be applied to realistically model genetic networks in cells.

% subsection diffusion-crowded (end)

% section conclusions (end)

% chapter results (end)

\appendix

\chapter{Acknowledgments} % (fold)
\label{sec:acknowledgements}
\todo{Needs to go before abstract}

I would like to thank my advisor, Professor Jorge Viñals, for his helpful guidance throughout this project and for his critical assessments of my computational approaches and algorithms. I also thank Professor Vincent Noireaux and Professor Yiannis Kaznessis for reviewing this thesis.

Finally, I would like to thank the authors and contributors of the IPython interactive scientific computing system~\cite{PER-GRA:2007} (and related projects), which has been extremely useful in developing and testing my code and in exploring, visualizing and interpreting the resulting data.
% section acknowledgements (end)

\chapter{Technical Notes} % (fold)
\label{sec:tech-notes}
\todo{Include Python version, along with versions of NumPy, SciPy, Matplotlib, IPython, etc. used. Maybe specifically call out the pseudorandom number generator used (NumPy's Mersenne twister).}

\todo{How to get code and data}
% section tech-notes (end)

\end{doublespacing}

\bibliographystyle{apsrev}
\bibliography{citations.bib}

\end{document}

