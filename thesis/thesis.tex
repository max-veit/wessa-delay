\documentclass[english,letterpaper,12pt]{article}
%\usepackage[margin=1in]{geometry}

% Encoding, fonts, and language (fold)
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2212}{\textminus}
\usepackage[T1]{fontenc}
\usepackage[pdftex,
            pdfauthor={Max Veit},
            pdftitle={Simulation of Genetic Regulatory Networks},
            pdfkeywords={stochastic simulation genetic regulatory networks epigenetics}]{hyperref}
\usepackage{fouriernc}
\usepackage{tgschola}
\usepackage{babel}
% (end)

% Mathematics and symbols (fold)
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{siunitx}
% (end)

% Figures (fold)
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
%\usepackage[font=scriptsize, it]{caption}
\usepackage{placeins}
% (end)

% Gnuplot vector images (fold)
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}
% (end)

% Source code listings (fold)
\usepackage{listings}
\usepackage{algorithmic}
% (end)

% Text-level formatting (fold)
\usepackage{color}
\usepackage{setspace}
\usepackage{multicol}
\frenchspacing
\usepackage[square,numbers]{natbib}
\sisetup{per-mode=symbol-or-fraction}
%\numberwithin{equation}{section}
% (end)

% Convenience / typographical consistency
\newcommand{\defkeywd}[1]{\textbf{#1}}
\usepackage[enable]{easy-todo}
% Deprecated - don't play nice with latex-suite autocomplete
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}

% Custom math symbols, commands
\newcommand{\tenexp}[1]{\times10^{#1}}
\newcommand{\dee}{\;\mathrm{d}}
\let\oldvec\vec
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\evec}[1]{\ensuremath{\vec{e}_{#1}}} % standard basis vector
\newcommand{\norm}[2]{\ensuremath{\|#1\|_{#2}}}
\newcommand{\bignorm}[2]{\ensuremath{\left\|#1\right\|_{#2}}}
\newcommand{\infnorm}[1]{\ensuremath{\|#1\|_\infty}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator{\Prob}{P}
% Physics Domain-Specific
\newcommand{\kB}{\ensuremath{k_\mathrm{B}}}
% Document-specific
\newcommand{\delaytime}{\ensuremath{\tau}}

% Headers and Footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Max Veit\\University of Minnesota}
\rhead{Simulation of Genetic\\Regulatory Networks}
\chead[]{}
\cfoot{\thepage}
\setlength{\headheight}{27.7pt}

\begin{document}
\title{Stochastic Simulation of Genetic Regulatory Networks with Delayed Reactions}
\author{Max Veit}
\date{5 May 2014}

% ----- General Writing TODO notes ----- %
% Fill in all references as soon as possible
% Ensure tense consistency
% Bring everything up to AIP's style-guide standards
% See where I can change sentences to active voice
% -- Replace occurrences of "one" with the less-awkward royal "we"
% Watch for long prepositional phrases

%TODO Format the title page according to UHP requirements (including acknowledgements, non-technical summary, etc.
\maketitle

% May not be necessary, but nice to have for now.
\tableofcontents

\begin{doublespacing}

\section{Introduction} % (fold)
\label{sec:introduction}

%TODO Background references look a little thin - maybe add some more from browsing around?
Recent research in biological physics~\cite{ecoli-decision} indicates that the behavior and internal workings of a living cell are much more rich and complex than the bare instructions coded into its DNA would suggest. For example, an individual section of DNA can be turned off when a repressor protein binds to the beginning of that section. Such mechanisms provide cells a way to change the expression of their DNA, i.e. to control which proteins are produced from their genes and in what amounts, an ability known as (genetic) transcriptional regulation. This ability allows genetically identical cells to adapt their behavior to different environments or to differentiate into different types of cells as in the development of a multicellular organism. Developing models for the selective expression of genes is key to understanding how cells perform their daily functions and respond to their enviroments.

%TODO Ref on biological circuitry would be helpful
Another interesting feature of transcriptional regulation is the presence of feedback. Even the simplest cells (such as \textit{E. Coli}, a commonly-studied model organism in biological physics) have hundreds of individually switchable sets of  genes \cite{ecoli-operons}.  This complexity allows genetic switches to interact in interesting and useful ways, forming a genetic regulatory network. An example would be two regions of DNA that each code for proteins that suppress the other region, although much more complex feedback mechanisms are found in real cells as well as synthesized in laboratories. \todo{more examples besides those below?} Such mechanisms have the potential to be harnessed as a form of biological circuitry - computation done with chemical reactions instead of electricity~\cite{bio-circuits}. Researchers have already synthesized basic components, such as oscillators~\cite{synth-osc}. Biological circuitry promises to deliver a level of control over cells that would allow bacteria to be harnessed for producing chemicals or fulfilling other useful roles in the body. Advancement of this field, however, depends on a better understanding of genetic networks and their behavior.

Many diverse strategies have been employed to model genetic regulatory networks~\cite{review-in-numero}\cite{bistable-modeling}\cite{gillespie-ssa}\cite{langevin-limit}. Many such models \todo{examples} approximate genetic regulation as a deterministic process. However, in very small systems (such as cells) where the effects of the finiteness of molecular populations become apparent, thermal fluctuations would be expected in inject a relatively large amount of stochasticity into the process. Recent experiments and theoretical considerations~\cite{ecoli-decision}\cite{stoch-theories}\cite{stoch-single-cell} make it clear that this stochasticity has a major influence on the function of these networks. Accurate models must therefore account for this stochasticity in some systematic way. \todo{Mention something about system size expansion?}

The method explored in this work is to directly simulate the sequence of chemical reactions occurring in the cell using a Monte Carlo algorithm that naturally accounts for the stochasticity present in small systems. The algorithm is known as the Gillespie stochastic simulation algorithm (SSA) and has seen use before in the context of genetic networks~\cite{we-chemkin}\cite{stoch-sys-bio}. The aim of this work is to extend the SSA to make it more practical for simulating real-world (natural or synthetic) genetic regulatory networks in order to analyze their behavior.

% section Introduction (end)

\section{Stochastic Chemical Kinetics} % (fold)
\label{sec:chemkin}

In order to study the behavior of genetic networks on a molecular level, they are usually modeled as chemical systems evolving under sets of coupled chemical reactions. The reactions represent actions such as the binding of repressors to DNA sites, protein production, and protein degradation. This representation allows genetic networks to be studied from the perspective of chemical kinetics, which seeks to understand the time evolution of the concentrations of the reactants in a system. In the case of genetic networks, the reactants are the proteins that characterize the dynamics of the genetic network. The \defkeywd{state} of a chemical system refers to the set of concentrations of all the reactants -- in this case, all the proteins, mRNA, and other biological molecules one is interested in -- involved in the system. As a chemical system evolves in time, it moves through the \defkeywd{state space} whose coordinates correspond to each of the individual concentrations.

%TODO Conflates discreteness with stochasticity - is there a need to distinguish the two?
In theories, such as reaction-rate equations (RRE), where the continuum limit is used the concentration of every reactant is assumed to be a continuuous variable. This limit is only valid for large systems, that is, in systems where the smallest change possible in the concentration of any reactant $X$ (the difference caused by adding or removing one molecule of $X$) is negligibly small relative to its average concentration $\bar{x}$. In small systems, i.e. in systems where the above limit is violated, the concentration of the reactants must be treated as discrete and the RRE formalism no longer gives a good description of the chemical system.

%TODO Bit of a weak intro for stochastic chemkin - see if there's a more interesting way to bring it in
The theory of stochastic chemical kinetics explicitly treats the discreteness and stochasticity present in small systems. It avoids solving for a \emph{deterministic} trajectory describing how the reactants evolve in time, as the RRE method does. Instead, it attempts to find the \emph{probability} that the chemical system will be be in a given state at a given time. It does this by employing the fundamental assumption that the probability that a given reaction $R_j$ will occur within the system volume in the next infinitesimal time interval of length $\dee t$ depends only on the current state $\vec{x}$. This probability is written as $a_j(\vec{x}) \dee t$, where the function $a_j(\vec{x})$ is known as the \defkeywd{propensity function} \cite{gillespie-ssa}. In other words, the fundamental assumption of chemical kinetics is that the chemical system can be represented as a (continuous-time) Markov chain.

Using the above assumption, it is possible to derive a differential equation that describes the time evolution of the probability distribution of the system. This probability distribution is written $\Prob(\vec{x}, t | \vec{x}_0, t_0)$, which means ``the probability, given that the system started in the state $\vec{x}_0$ at time $t_0$, that the system will be in the state $\vec{x}$ at some later time $t$.'' The equation is called the chemical master equation, and in the form given in \cite{gillespie-ssa}, it reads:
\begin{equation}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t | \vec{x}_0, t_0) = \sum_j \left( a_j (\vec{x} - \vec{s}_j) \Prob(\vec{x} - \vec{s}_j, t | \vec{x}_0, t_0) - a_j(\vec{x}) \Prob(\vec{x}, t | \vec{x}_0, t_0) \right)
    \label{eq:master-eqn}
\end{equation}
The sum runs over all the reaction pathways in the chemical system. The vector $\vec{s}_j$, known as the state-change vector of reaction $j$, indicates the effect of the reaction $j$ on the state of the chemical system: reaction $j$ takes the state instantaneously from $\vec{x}$ to $\vec{x} + \vec{s}_j$.

The above definition of propensity and the resulting Master equation rely on several simplifying assumptions about the system. The most severe of these is that the chemical system is assumed to be a homogeneous (well-stirred) ideal gas or dilute solution. \todo{may need reference that details assumptions, since Gillespie (2007) doesn't mention the ideal-gas part} While these assumptions begin to break down in living cells (see Section~\ref{sub:diffusion-crowded}), the utility and ease of implementation of algorithms to solve the chemical master equation have led to wide use of this theory in modeling the biochemistry of cells~\cite{we-chemkin}\cite{stoch-sys-bio}.

\subsection{Propensity Functions} % (fold)
\label{sub:propensities}

The propensity functions defined above have different forms depending on the order of the associated reactions, i.e. whether they are production (zero-order), unimolecular, bimolecular, or higher-order reactions \cite{gillespie-ssa}. The order of the reaction refers to the number of separate molecules that constitute the inputs, or reactants. A production reaction, i.e. a reaction with no input reactants, can simply be modeled by using a constant propensity $a_j^{(0)} = c_j^{(0)}$. 

For a unimolecular reaction, e.g. a reaction $R_j$ taking one molecule of $X_1$ to some products, the probability that an isolated molecule of $X_1$ will undergo the reaction in the next infinitesimal time interval $\dee t$ can be assumed to be a constant $c_j^{(1)} \dee t$. The probability of \emph{any} reaction $R_j$ occurring within the system volume in the time interval $\dee t$ is thus proportional to the number of molecules of $X_1$ in the system, so $a_j^{(1)}(\vec{x}) = c_j^{(1)} x_1$.

For a bimolecular reaction, the propensity is the probability (per unit time) that any pair of reactant molecules will collide within the system volume, multiplied by the probability that such a collision will actually result in a reaction. One could therefore assume the probability per unit time of any pair of molecules reacting is a constant $c_j^{(2)}$, implying the propensity function is proportional to the number of pairs of reactants in the system volume. If the two reactant molecules are of different species $X_1$ and $X_2$, the propensity $a_j^{(2)}(\vec{x}) = c_j^{(2)} \cdot x_1 x_2$. If they are of the same species $X_1$, $a_j^{(2)}(\vec{x}) = c_j^{(2)} \cdot \frac{1}{2}x_1(x_1 - 1)$ (which comes from $c_j^{(2)}\binom{x}{2}$, where $\binom{n}{k}$ denotes the binomial coefficient).

\todo{Discuss higher-order reactions? Gillespie says they don't technically exist, but some theoretical models use them.}

% subsection propensities (end)

\subsection{Delayed Reactions} % (fold)
\label{sub:delayed-reactions}

In order to reduce the computational cost and model complexity of applying the SSA to biological systems, an additional abstraction is adopted. Processes common in cell biology, such as DNA replication, protein production, and protein digestion, actually consist of sequences of thousands of individual chemical reactions like the binding of individual nucleotides to a developing RNA strand. It would be tedious and expensive to simulate each individual step of the process. More importantly, it would be extremely wasteful to do so if one is only interested in the high-level dynamics of proteins and genes, not the details of the constituent process.

One can avoid simulating each step of a complex biological process by modeling the entire process as a single reaction. For example, a protein-production reaction (which itself consists of many complex multi-step processes such as RNA transcription and protein folding) can be abstracted as a single reaction that produces a protein from nothing. However, this reaction cannot be said to occur instantaneously (as with simple chemical reactions), as the entire process it represents requires a certain duration to complete. Experimental evidence indicates that the duration of biologically important reactions such as protein production can be on the order of minutes to hours. Since this duration is long compared to the timescales of other cellular processes, it is important to account for it in the modeling of genetic networks \cite{delay-oscillations}.

To account for this duration, one can associate a \defkeywd{delay} with the reaction to represent the time the underlying process needs to complete. In effect, the propensity function for the delayed reaction depends not on the current state, but on the history of the chemical system. More precisely, if reaction $j$ is delayed with a time $\delaytime_j$, its propensity function is written $a_j \left(\vec{x}(t - \delaytime_j) \right)$. For simplicity, $\delaytime_j$ is assumed to be a number. In general, however, the delay may be better characterized by a probability distribution.

Accounting for the presence of delays, Equation~\eqref{eq:master-eqn} can be written: \todo{check for correctness}
\begin{multline}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t | \vec{x}_0, t_0) = \\
    \sum_j \left( a_j (\vec{x}(t - \delaytime_j) - \vec{s}_j) \Prob(\vec{x}(t - \delaytime_j) - \vec{s}_j, t | \vec{x}_0, t_0) - \right.\\
    \left. a_j(\vec{x}(t - \delaytime_j)) \Prob(\vec{x}(t - \delaytime_j), t | \vec{x}_0, t_0) \right)
    \label{eq:master-eqn-delay}
\end{multline}
in the case where each delay $\tau_j$, which may be zero, is a single number.

Finally, introducing delayed reactions has an important consequence: Since delayed reactions depend on the history of the chemical system, models incorporating them are non-Markovian. This fact has important consequences in the simulation of systems with delayed reactions. These consequences are explored in Section~\ref{sub:we-delays}.

% subsection delayed-reactions (end)

% section chemkin (end)

\section{Methodology} % (fold)
\label{sec:methodology}

\subsection{Gillespie Stochastic Simulation Algorithm} % (fold)
\label{sub:gillespie-ssa}

One of the most well-known and widely used \todo{back up with refs} algorithms for simulating stochastic chemical systems is the stochastic simulation algorithm (SSA), which was introduced in 1976 by Daniel Gillespie~\cite{gillespie-1976}. As a Monte Carlo technique, it does not attempt to solve the chemical master equation \eqref{eq:master-eqn} explicitly. Rather, the approach taken by the SSA is to generate a \defkeywd{trajectory}, which is the path $\vec{x}(t)$ that one possible \emph{instance}, or example, of the chemical system might take through the state space given some initial conditions $\vec{x}(t_0) = \vec{x}_0$. Unlike the deterministic trajectories found using RRE, these trajectories are generated probabilistically. In effect, the SSA simulates the time evolution of a single instance of the chemical system with some arbitrary initial conditions.

The trajectories generated by the SSA can be seen as samples of the underlying probability distribution $\Prob(\vec{x}, t | \vec{x}_0, t_0)$ that describes the chemical system. In principle, one can generate a very large number of samples (trajectories) in order to estimate a distribution that converges to the true one.

The SSA generates trajectories (with the initial conditions $\vec{x}(t_0) = \vec{x_0}$) by repeating the following steps for each iteration $n$ \cite{gillespie-ssa}:
\begin{enumerate}
    \item Compute the propensities $a_j(\vec{x}_n)$ for all reactions $j$ and their sum $a_\text{tot}(\vec{x}_n)$ (see Section~\ref{sub:propensities}).
    \item Choose the next reaction type and the waiting time until that reaction from the following probability distributions:
    \begin{itemize}
        \item Waiting time: $\Prob(t_w)\dee t_w = a_\text{tot}(\vec{x}_n) \exp(-a_\text{tot}(\vec{x}_n) t_w) \dee t_w$
        \item Reaction type: $\Prob(j) = a_j(\vec{x}_n) / a_\text{tot}(\vec{x}_n)$
    \end{itemize}
    \item Update the current state and time:
    \begin{itemize}
        \item $t_{n+1} = t_n + t_w$
        \item $\vec{x}_{n+1} = \vec{x}_n + \vec{s}_a$, where $\vec{s}_a$ is the state-change vector for the reaction chosen above
    \end{itemize}
\end{enumerate}
The iteration is continued typically until the time $t$ reaches (exceeds) a predefined stop time. Since chemical systems \todo{in the CME/SSA approximation} remain in their current state until another reaction occurs, the sequence $\left(\vec{x_n}, t_n\right)_{n=1}^N$ can be interpreted as the continuous-time trajectory $\vec{x}(t)$ of the instance.

\todo{Explain physical origin of probability distributions}

% subsection gillespie-ssa (end)

\subsection{Extension to Non-Markovian Dynamics} % (fold)
\label{sub:non-markovian}

The classic SSA was designed only for explicitly Markovian chemical systems. However, one could imagine extending the algorithm to include delayed reactions. One such extension is proposed by \cite{delay-oscillations}. The modified algorithm, upon selecting a delayed reaction in Step~2 (see Section~\ref{sub:gillespie-ssa}), schedules the reaction to fire at a later time (that is, Step~3 is postponed by a time $\tau_a$). \todo{Mention what's wrong with this approach?}

In this work, a method that more closely follows the analytical modeling of delayed reactions via Equation~\eqref{eq:master-eqn-delay} is chosen. The modification is made in the propensity calculation of Step~1: To calculate the propensity of a reaction $R_j$ with delay $\delaytime_j$, one simply uses the state from $\delaytime_j$ time units ago. Thus, with a trajectory at time $t$, the propensity would be computed as $a_j = a_j\left(\vec{x}(t - \delaytime_j)\right)$. This is consistent with the delayed-reaction formulation of the master equation, Equation~\eqref{eq:master-eqn-delay}.

One undesirable feature is introduced by including delayed reactions using either method: Since the delayed reactions lag behind the current state, it is possible for individual concentrations to go below zero, an obviously unphysical result. In the non-delayed SSA, any reaction that could decrease the concentration $x_i$ has a propensity proportional to that concentration (to some power). Therefore, when $x_i$ goes to zero, any reaction that could decrease $x_i$ has a zero propensity and is blocked from running. A delayed reaction, however, does not ``notice'' zero concentrations in real time so it is not blocked in time to avoid decreasing concentrations from zero\footnote{Some analytical models of delayed systems have this problem owing to the difficulty of analytically constraining concentrations to be nonnegative. The result is often nonsensical behavior, such as oscillations with exponentially increasing amplitude.}.

One solution is to manually impose the constraint $x_i \geq 0\; \forall i$, which is much easier to do within the SSA than in analytical models. The method used in this work is to manually block the offending reactions: In Step~2 of the SSA, if a reaction is selected that would make any individual concentration go negative, the reaction is discarded and another is selected. (This amounts to manually setting that reaction's propensity to zero.) \todo{May change code to actually set propensity to zero - stay tuned}

% subsection non-markovian (end)

\subsection{Weighted-Ensemble Resampling} % (fold)
\label{sub:we-resampling-intro}

The most straightforward way to obtain a probability distribution using the SSA is to run an ensemble of trajectories in parallel, then estimate the probability density $P(\vec{x}, t | \vec{x}_0, t_0)$ from those samples\footnote{If the probability distribution is known to be steady-state, i.e. independent of time, then one can simply average a single trajectory over a long period of time to obtain $P(\vec{x})$. However, in systems where time-independence is not known \textit{a priori}, the ensemble method must be used.}.

The main issue with this method is that it generally samples state space unevenly. Trajectories in the ensemble tend to congregate, by construction, in the most probable regions of state space while less probable regions (the ``tails'' of the distribution) are more rarely visited. The result is that the accuracy of the sample of the probability distribution obtained using this simple ensemble increases with the probability density at the same location. This effect makes this method extremely inefficient if one is most interested in the least probable regions, as is the case in many problems in chemistry and biology (e.g. computing transition rates).
\todo{Examples relating to genetic networks? Bistable/Schl√∂gl systems, maybe?}
In many cases, \todo{Illustrative example?} one would need to use a prohibitively large number of trajectories to adequately sample the improbable transition regions.
\todo{Include illustration if appropriate}

To more uniformly and efficiently sample the phase space, one can use a method known (in the context of molecular dynamics and stochastic simulation) as the \defkeywd{weighted ensemble}~\cite{we-orig}, a type of importance sampling. The basic strategy is to periodically redistribute the samples in a way that does not change the final estimate of the probability distribution $P(\vec{x}, t | \vec{x}_0, t_0)$. This is achieved by assigning each trajectory a \defkeywd{weight} $w_k$ such that the sum of all weights is always 1. In effect, the method biases the underlying distribution so that rare events are sampled as frequently as the common ones while keeping track of weights in order to obtain a sample of the original, \emph{unbiased} probability distribution.

The weighted-ensemble method has been applied to stochastic chemical kinetics before; see~\cite{we-chemkin}. The method used in this work generally follows the version presented in the paper. The overall procedure is as follows: First, choose an initial ensemble of $P$ trajectories. Then, repeat the following steps as many times as desired:
\begin{enumerate}
    \item Perform a resampling step to equalize the distribution of trajectories over state space.
    \item Run all the trajectories independently for a time $\tau_p$.
\end{enumerate}

The method implements the resampling step by partitioning the state space into discrete \defkeywd{bins}. The goal of the resampling step is to equalize the number of trajectories in each bin so that the trajectories are distributed more uniformly over state space. The equalization is done using two basic operations: To increase the number of trajectories in a bin, a trajectory $T_a$ can be replicated (or ``split'') to obtain $M$ copies. Each of the copies receives a weight $w_a/M$; in this way, the total weight within a bin is conserved. To decrease the number of trajectories, a trajectory $T_b$ can be deleted (or ``killed'') and its weight assigned to another trajectory $T_c$ in the same bin, i.e. $T_c$ receives a new weight $w_b + w_c$. Again, the total weight within each bin is conserved.



As the bins are statically defined, the estimate of the probability distribution is taken to be a histogram over the bins. Many different binning strategies are possible (see \cite{we-exact} for examples), but a simple uniform 

\todo{Explain how resampling is done, make analogies to Monte Carlo variance reduction}

\todo{WE with SSA, conceptual issues. Resolution with Markovian property.}

% subsection we-resampling-intro (end)

\subsection{Weighted Ensemble With Delays} % (fold)
\label{sub:we-delays}

\todo{Invalidity of resolution w/ Markovian property.}

% subsection we-delays (end)

\subsection{Verification Test Cases} % (fold)
\label{sub:verification}

% System for which analytical distribution is known

% Simple production-degradation system

% subsection verification (end)

% section Methodology (end)

\section{Delayed-Degradation Model System} % (fold)
\label{sec:delayed-deg}

% section delayed-deg (end)

\section{Simple Feedback System} % (fold)
\label{sec:experimental-system}

% section experimental-system (end)

\section{Conclusions and Future Work} % (fold)
\label{sec:conclusions}

% Future Work
\subsection{Modeling Crowded Environments} % (fold)
\label{sub:diffusion-crowded}

% subsection diffusion-crowded (end)

%     Crowded environments

% section conclusions (end)

\end{doublespacing}

\appendix

\bibliographystyle{apsrev}
\bibliography{citations.bib}

\end{document}

