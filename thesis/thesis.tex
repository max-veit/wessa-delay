\documentclass[english,letterpaper,12pt]{report}
%\usepackage[margin=1in]{geometry}

% Encoding, fonts, and language (fold)
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2212}{\textminus}
\usepackage[T1]{fontenc}
\usepackage[pdftex,
            pdfauthor={Max Veit},
            pdftitle={Simulation of Genetic Regulatory Networks},
            pdfkeywords={stochastic simulation genetic regulatory networks epigenetics}]{hyperref}
\usepackage{fouriernc}
\usepackage{tgschola}
\usepackage{babel}
% (end)

% Mathematics and symbols (fold)
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{siunitx}
% (end)

% Figures (fold)
\usepackage{graphicx}
\usepackage{float}
\usepackage[font={small,it}]{caption}
\usepackage{subcaption}
\usepackage{placeins}
% (end)

% Gnuplot vector images (fold)
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}
% (end)

% Source code listings (fold)
\usepackage{listings}
\usepackage{algorithmic}
% (end)

% Text-level formatting (fold)
\usepackage{color}
\usepackage{setspace}
\usepackage{multicol}
\frenchspacing
\usepackage[square,numbers]{natbib}
\sisetup{per-mode=symbol-or-fraction}
%\numberwithin{equation}{section}
% (end)

% Convenience / typographical consistency
\newcommand{\defkeywd}[1]{\textbf{#1}}
\usepackage[enable]{easy-todo}
% Deprecated - don't play nice with latex-suite autocomplete
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}

% Custom math symbols, commands
\newcommand{\tenexp}[1]{\times10^{#1}}
\newcommand{\dee}{\;\mathrm{d}}
\let\oldvec\vec
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\evec}[1]{\ensuremath{\vec{e}_{#1}}} % standard basis vector
\newcommand{\norm}[2]{\ensuremath{\|#1\|_{#2}}}
\newcommand{\bignorm}[2]{\ensuremath{\left\|#1\right\|_{#2}}}
\newcommand{\infnorm}[1]{\ensuremath{\|#1\|_\infty}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}
\DeclareMathOperator{\Prob}{P}
% Physics Domain-Specific
\newcommand{\kB}{\ensuremath{k_\mathrm{B}}}
% Document-specific
\newcommand{\delaytime}{\ensuremath{\tau}}

% Headers and Footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Max Veit\\University of Minnesota}
\rhead{Simulation of Genetic\\Regulatory Networks}
\chead[]{}
\cfoot{\thepage}
\setlength{\headheight}{27.7pt}

\begin{document}
\title{Stochastic Simulation of Genetic Regulatory Networks with Delayed Reactions}
\author{Max Veit}
\date{9 May 2014}

% ----- General Writing TODO notes ----- %
% Fill in all references as soon as possible
% Ensure tense consistency
% Bring everything up to AIP's style-guide standards
% See where I can change sentences to active voice
% -- Replace occurrences of "one" with the less-awkward royal "we"
% Watch for long prepositional phrases

\pagenumbering{roman}
\begin{titlepage}
    \makeatletter
    \begin{center}
        \vspace{1in}

        \LARGE \@title

        \vspace{0.5in}

        \large \@author

        \vspace{1in}

    \end{center}
    \normalsize
        Submitted under the supervision of Jorge Viñals to the University Honors Program at the University of Minnesota--Twin Cities in partial fulfillment of the requirements for the degree of Bachelor of Science, \textit{summa cum laude} in Physics.
    \begin{center}

        \vspace{0.5in}

        \large \@date
    \end{center}
    \makeatother
\end{titlepage}
 
\begin{doublespacing}

\chapter*{Acknowledgments} % (fold)
\label{sec:acknowledgements}

I would like to thank my advisor, Professor Jorge Viñals, for his helpful guidance throughout this project and for his suggestions for improving my computational approaches and algorithms. I additionally thank Professor Vincent Noireaux and Professor Yiannis Kaznessis for reviewing this thesis and offering useful comments.

I am also grateful to my academic advisers in the University Honors Program and the School of Physics and Astronomy. Their advice and support over the past four years was instrumental in helping me explore my academic interests and find a project that appealed to those interests. 

Finally, I would like to thank the authors and contributors of the open software projects Python, IPython, SciPy, Matplotlib, and \LaTeX, whose software allowed me to quickly and easily implement algorithms, test ideas, explore and visualize results, and professionally communicate them in this thesis.
% section acknowledgements (end)

\chapter*{Abstract}
Many important processes in cells are controlled by genetic regulatory networks. To accurately model such networks, it is often necessary to include reactions with delays. In this work I apply the weighted ensemble (WE) method to simulate models of genetic regulatory networks that incorporate delays. In order to accurately capture the discreteness and stochasticity present in small systems, the Gillespie stochastic simulation algorithm (SSA), extended to include delayed reactions, is used to evolve trajectories in time. Tests of this method on two simple model systems show that the WE method yields an unbiased estimate of the system's probability distribution in the presence of delays, despite the SSA's non-uniform timesteps. I additionally use the extended SSA to investigate the assumptions used in analytical models of the simple delayed-degradation system. The numerical results indicate that a mean-field approximation is not justified near the system's bifurcation point, but is conditionally justified both in the limit of small and (surprisingly) of large propensity of the delayed reaction.

\section*{Non-technical Summary}
Biologists and biophysicists have observed that living cells can behave in ways that cannot be explained only by the genes encoded in their DNA. Put another way, two separate cells with exactly the same genetic code can behave in different ways. Cells can effectively make decisions, turning genes on and off in response to their surroundings. In this work I present a computational algorithm that can be used to simulate systems of interacting genetic switches to better understand how they function. I test the algorithm on two simplified model systems.

\pagebreak[4]

\end{doublespacing}

\tableofcontents

\begin{doublespacing}

\chapter{Introduction} % (fold)
\label{sec:introduction}

\pagenumbering{arabic}

%TODO Background references look a little thin - maybe add some more from browsing around?
The behavior and internal workings of a living cell are much more rich and complex than the bare instructions coded into its DNA would suggest. Multicellular organisms like plants and animals are the most recognizable examples: They consist of immense numbers of cells with essentially identical sets of genes, yet each cell is specialized to perform one of a huge variety of tasks~\cite{grn-review}. Even simple organisms like bacteria can decide which genes to express, or turn into proteins, based on environmental factors~\cite{ecoli-decision}. \todo{Mention more examples in passing} The common theme is that a cell can control the expression of its genes by binding specific proteins to matching sites on its DNA, either blocking or enabling that gene's expression.

The proteins that participate in genetic regulation are produced from different genes which themselves could be regulated, so genetic switches are in general coupled to one another. The interaction of multiple genetic switches, which can number in the hundreds even in simple cells (like \textit{E. Coli}, a commonly-studied model organism in biophysics)~\cite{ecoli-operons}, form what is known as a \defkeywd{genetic regulatory network}. Such networks regulate functions ranging from cellular differentiation~\cite{grn-review} to daily circadian rhythms~\cite{circ-rhythm-review} in complex multicelluar organisms. It is important to develop accurate and general mathematical models of genetic networks~\cite{review-in-numero}. Not only is a better fundamental understanding of cellular processes worthwhile in its own right, but it would help diagnose and treat the disorders that could arise if genetic networks function improperly.

Another relatively new application of modeling genetic networks is in synthetic biology, the development of artificial cells for a variety of tasks such as producing useful chemicals like drugs or fuels, as well as delivering drugs in a targeted manner~\cite{synth-bio-applications}. In order to perform these functions, artificial cells will require artificial genetic networks. Basic components of these networks, such as synthetic clocks, have already been synthesized in laboratories~\cite{synth-osc}. Further advancement of this field, however, also depends on a better understanding of genetic networks and their behavior.

Many diverse strategies have been employed to model genetic regulatory networks~\cite{review-in-numero}\cite{bistable-modeling}\cite{gillespie-ssa}\cite{cme-closure}. Often genetic regulation is approximated as a deterministic process. However, in very small systems (such as cells), stochastic thermal fluctuations in the concentrations of molecules can be expected to be large relative to their averages. It is becoming clear, with evidence from both theory and experiment, that stochasticity is a fundamental feature and often a driving force in the function of genetic regulatory networks~\cite{ecoli-decision}\cite{stoch-theories}\cite{stoch-single-cell}\cite{delay-oscillations}. In particular, there is a large amount of variation in genetic expression between individual cells in genetically identical colonies.

One way of accounting for this stochasticity is a technique known as system size expansion, which treats the concentrations of chemical species as continuous quantities without any restriction that they be nonnegative \cite{langevin-limit}\cite{delayed-deg-notes}. However, these models break down in a fundamental way when considering very small numbers of molecules, where the inherent discreteness of molecular concentrations plays an important role.

The method explored in this work is to directly simulate the sequence of chemical reactions occurring in the cell using a Monte Carlo algorithm that naturally accounts for the stochasticity in discreteness present in small systems. The algorithm is known as the Gillespie stochastic simulation algorithm (SSA) and is widely used to model genetic networks~\cite{we-chemkin}\cite{stoch-sys-bio}. The aim of this work is to extend the SSA to make it more practical for simulating real-world (natural or synthetic) genetic regulatory networks in order to analyze their behavior.

% section Introduction (end)

\section{Stochastic Chemical Kinetics} % (fold)
\label{sec:chemkin}

In order to study the behavior of genetic networks on a molecular level they are usually modeled as \defkeywd{chemical systems}, bounded volumes containing molecules evolving under sets of coupled chemical reactions. The reactions represent actions such as the binding of repressors to DNA sites, protein production, and protein degradation. This representation allows genetic networks to be studied from the perspective of chemical kinetics, which seeks to understand the time evolution of the concentrations of the reactants in a system. In the case of genetic networks, the reactants are the proteins that characterize the dynamics of the genetic network. The \defkeywd{state} of a chemical system refers to the set of concentrations (or equivalently, for a fixed system volume, populations) of all the reactants -- in this case, all the proteins, mRNA, and other biological molecules one is interested in -- involved in the system. As a chemical system evolves in time, it moves through the \defkeywd{state space} whose coordinates correspond to each of the individual concentrations.

One approach to modeling chemical systems is to go to the continuum limit, i.e. a very large (macroscopic) system where the effects of discreteness and thermal fluctuations are negligible. Specifically, this limit assumes that the smallest change possible in the concentration of any reactant $X$ (the difference caused by adding or removing one molecule of $X$) is negligibly small relative to its average concentration $\bar{x}$. One may also assume in this limit that the thermal fluctuations in the concentration are negligible compared to the average, so each concentration can be treated as a \emph{deterministic} quantity. This second assumption is the basis of the formalism of reaction-rate equations (RRE), which specifies how to solve for the concentrations using a set of deterministic differential equations. 

Another technique, known as system-size expansion, makes the assumption of continuous concentrations but not of negligible fluctuations. The zeroth order of this expansion is equivalent to RRE. The first order is known as the Langevin limit; in this limit, it is possible to describe the extent of the thermal fluctuations from the RRE concentrations \cite{langevin-limit}. However, neither theory is an exact description of very small chemical systems where discreteness and the constraint of non-negative molecular populations play an important role.

The theory of stochastic chemical kinetics explicitly treats the discreteness and stochasticity present in these systems. It avoids solving for a deterministic trajectory describing how the reactants evolve in time, as the RRE method does. Instead, it attempts to find the \emph{probability} that the chemical system will be be in a given state at a given time.

It does this by making several simplifying assumptions about the system. Namely, it assumes that the chemical system is a homogeneous (well-stirred) ideal gas or dilute solution in thermal equilibrium~\cite{langevin-limit}. While this assumption begins to break down in living cells (see Section~\ref{sub:diffusion-crowded}), the utility and ease of implementation of algorithms to simulate or solve the chemical master equation have led to wide use of this theory in modeling the biochemistry of cells~\cite{we-chemkin}\cite{stoch-sys-bio}.

Under the above condition, we can assume that the probability that a given reaction $R_j$ will occur within the system volume in the next infinitesimal time interval of length $\dee t$ depends only on the current state $\vec{x}$. This probability is written as $a_j(\vec{x}) \dee t$, where the function $a_j(\vec{x})$ is known as the \defkeywd{propensity function} \cite{gillespie-ssa}. In other words, the fundamental assumption of chemical kinetics is that the chemical system can be represented as a (continuous-time) Markov chain.

Using this definition of propensity, it is possible to derive a differential equation that describes the time evolution of the probability distribution of the system. This probability distribution is written $\Prob(\vec{x}, t | \vec{x}_0, t_0)$, which means ``the probability, given that the system started in the state $\vec{x}_0$ at time $t_0$, that the system will be in the state $\vec{x}$ at some later time $t$.'' The equation is called the chemical master equation, and in the form given in \cite{gillespie-ssa}, it reads:
\begin{equation}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t | \vec{x}_0, t_0) = \sum_{j=1}^N \left( a_j (\vec{x} - \vec{s}_j) \Prob(\vec{x} - \vec{s}_j, t | \vec{x}_0, t_0) - a_j(\vec{x}) \Prob(\vec{x}, t | \vec{x}_0, t_0) \right)
    \label{eq:master-eqn-gillespie}
\end{equation}
The sum runs over all $N$ reaction pathways in the chemical system. The vector $\vec{s}_j$, known as the state-change vector of reaction $j$, indicates the effect of the reaction $j$ on the state of the chemical system: reaction $j$ takes the state instantaneously from $\vec{x}$ to $\vec{x} + \vec{s}_j$.

The first term under the sum can be seen as a source term; it represents the influx of probability into state $\vec{x}$ caused by the reaction $R_j$ taking the state $\vec{x} - \vec{s}_j$ to the state $\vec{x}$. Similarly, the second term can be seen as a drain term, representing the state change from $\vec{x}$ to $\vec{x} + \vec{s}_j$.

In order to simplify the above expression, we can average over all possible initial states $\vec{x}_0$ at time $t_0$ and write instead $P(\vec{x}, t)$. Additionally, we can write the equation more compactly by introducing the shift operator $\hat{T}^{\vec{s}_j}$, where $\hat{T}^{\vec{s}_j} P(\vec{x}, t) = P(\vec{x} + \vec{s}_j, t)$. The result is the more convenient form:
\begin{equation}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t) = \sum_{j=1}^N a_j(\vec{x})(\hat{T}^{-\vec{s}_j} - 1)\Prob(\vec{x}, t)
    \label{eq:master-eqn}
\end{equation}

\subsection{Propensity Functions} % (fold)
\label{sub:propensities}

The forms of the propensity functions defined above can be derived under the homogeneous ideal-gas assumption. It is from these derivations that we get the fundamental assumption of chemical kinetics, that the propensities depend only on the current state.

The derivations are different depending on the order of the associated reactions, i.e. whether they are production (zero-order), unimolecular, bimolecular, or higher-order reactions \cite{gillespie-ssa}. The order of the reaction refers to the number of separate molecules that constitute the inputs, or reactants. A production reaction, i.e. a reaction with no input reactants, can simply be modeled by using a constant propensity $a_j^{(0)} = c_j^{(0)}$. 

For a unimolecular reaction, e.g. a reaction $R_j$ taking one molecule of $X_1$ to some products, the probability that an isolated molecule of $X_1$ will undergo the reaction in the next infinitesimal time interval $\dee t$ can be assumed to be a constant $c_j^{(1)} \dee t$. The probability of \emph{any} reaction $R_j$ occurring within the system volume in the time interval $\dee t$ is thus proportional to the number of molecules of $X_1$ in the system, so $a_j^{(1)}(\vec{x}) = c_j^{(1)} x_1$.

For a bimolecular reaction, the propensity is the probability (per unit time) that any pair of reactant molecules will collide within the system volume, multiplied by the probability that such a collision will actually result in a reaction. One could therefore assume the probability per unit time of any pair of molecules reacting is a constant $c_j^{(2)}$, implying the propensity function is proportional to the number of pairs of reactants in the system volume. If the two reactant molecules are of different species $X_1$ and $X_2$, the propensity $a_j^{(2)}(\vec{x}) = c_j^{(2)} \cdot x_1 x_2$. If they are of the same species $X_1$, $a_j^{(2)}(\vec{x}) = c_j^{(2)} \cdot \frac{1}{2}x_1(x_1 - 1)$.

% subsection propensities (end)

\subsection{Delayed Reactions} % (fold)
\label{sub:delayed-reactions}

In order to reduce the computational cost and model complexity of applying the SSA to biological systems, an additional abstraction is adopted. Processes common in cell biology, such as DNA replication, protein production, and protein digestion, actually consist of sequences of thousands of individual chemical reactions like the binding of individual nucleotides to a developing RNA strand. It would be tedious and expensive to simulate each individual step of the process. More importantly, it would be extremely wasteful to do so if one is only interested in the high-level dynamics of proteins and genes, not the details of the constituent process.

We can avoid simulating each step of a complex biological process by modeling the entire process as a single reaction. For example, a protein-production reaction (which itself consists of many complex multi-step processes such as RNA transcription and protein folding) can be abstracted as a single reaction that produces a protein from nothing. However, this reaction cannot be said to occur instantaneously (as with simple chemical reactions), as the entire process it represents requires a nonzero duration to complete. Experimental evidence indicates that the duration of biologically important reactions such as protein production can be on the order of minutes to hours. Since this duration is long compared to the timescales of other cellular processes, it is important to account for it in the modeling of genetic networks \cite{delay-oscillations}.

To account for this duration, we can associate a \defkeywd{delay} with the reaction to represent the time the underlying process needs to complete. In effect, the propensity function for the delayed reaction depends not on the current state, but on the history of the chemical system. More precisely, if reaction $j$ is delayed with a time $\delaytime_j$, its propensity function is written $a_j \left(\vec{x}(t - \delaytime_j) \right)$. For simplicity, $\delaytime_j$ is assumed to be a number. In general, however, the delay may be better characterized by a probability distribution.

In the case where some of the reactions in a system are delayed, we can modify Equation~\eqref{eq:master-eqn} to:
\begin{align}
    \frac{\partial}{\partial t} \Prob(\vec{x}, t) &= \sum_{j=P+1}^N a_j(\vec{x})(\hat{T}^{-\vec{s}_j} - 1)\Prob(\vec{x}, t) \\
                                                  &+ \sum_{j=1}^P \sum_{\vec{x}'} H(\vec{x}) a_j(\vec{x}') (\hat{T}^{-\vec{s}_j} - 1) \Prob_2(\vec{x}, t;\: \vec{x}', t - \delaytime_j)
    \label{eq:master-eqn-delay}
\end{align}
where $P$ of the $N$ total reactions are delayed and each delay $\delaytime_j$ is assumed to be a single number. The second sum term involves an inner sum over all states $\vec{x}'$ in the state space; the joint probability distribution $\Prob_2(\vec{x}, t;\: \vec{x}', t - \delaytime_j)$ gives the probability of the system visiting state $\vec{x}'$ at time $t - \delaytime_j$ \emph{and} reaching state $\vec{x}$ at time $t$. The inner sum thus covers all possible paths the system could take to reach the current state $\vec{x}$ over the duration of the delay. The multidimensional step function $H(\vec{x})$ is included to block unphysical reactions, i.e. those that take any single concentration below zero (the value of $H(\vec{x})$ is one if all the $x_i$ are positive, zero otherwise). 

Finally, introducing delayed reactions has an important consequence: Since delayed reactions depend on the history of the chemical system, models incorporating them are non-Markovian. This fact severely limits the analytical tools one can apply to analyze the stochastic behavior of systems with delayed reactions; Section~\ref{sub:delayed-joint-dist} discusses the implications of non-Markovian dynamics for analytical models. It also has important consequences in the numerical simulation of delayed systems. These consequences are explored in Section~\ref{sub:we-delays}.

% subsection delayed-reactions (end)

% section chemkin (end)

\chapter{Methodology} % (fold)
\label{sec:methodology}

\section{Gillespie Stochastic Simulation Algorithm} % (fold)
\label{sub:gillespie-ssa}

One of the most well-known and widely used~\cite{gillespie-ssa}\cite{we-chemkin}\cite{stoch-sys-bio} algorithms for simulating small chemical systems is the stochastic simulation algorithm (SSA), which was introduced in 1976 by Daniel Gillespie~\cite{gillespie-1976}. As a Monte Carlo technique, it does not attempt to solve the chemical master equation \eqref{eq:master-eqn} explicitly. Rather, the approach taken by the SSA is to generate a \defkeywd{trajectory}, which is the path $\vec{x}(t)$ that one possible \emph{instance} of the chemical system might take through the state space given some initial conditions $\vec{x}(t_0) = \vec{x}_0$. Unlike the deterministic trajectories found using RRE, these trajectories are generated probabilistically. In effect, the SSA simulates the time evolution of a single instance of the chemical system with some arbitrary initial conditions.

The trajectories generated by the SSA can be seen as samples of the underlying probability distribution $\Prob(\vec{x}, t | \vec{x}_0, t_0)$ that describes the chemical system. In principle, one can generate a very large number of samples (trajectories) in order to estimate a distribution that converges to the true one.

The SSA generates trajectories (with the initial conditions $\vec{x}(t_0) = \vec{x}_0$) by repeating the following steps for each iteration $n$ \cite{gillespie-ssa}:
\begin{enumerate}
    \item Compute the propensities $a_j(\vec{x}_n)$ for all reactions $j$ and their sum $a_\text{tot}(\vec{x}_n)$ as described in Section~\ref{sub:propensities}.
    \item Choose the next reaction type and the waiting time until that reaction from the following probability distributions:
    \begin{itemize}
        \item Waiting time: $\Prob(t_w)\dee t_w = a_\text{tot}(\vec{x}_n) \exp(-a_\text{tot}(\vec{x}_n) t_w) \dee t_w$
        \item Reaction type: $\Prob(j) = a_j(\vec{x}_n) / a_\text{tot}(\vec{x}_n)$
    \end{itemize}
    \item Execute the reaction, i.e. update the current state and time:
    \begin{itemize}
        \item $t_{n+1} = t_n + t_w$
        \item $\vec{x}_{n+1} = \vec{x}_n + \vec{s}_a$, where $\vec{s}_a$ is the state-change vector for the reaction chosen above
    \end{itemize}
\end{enumerate}
The iteration is continued typically until the time $t$ reaches (exceeds) a predefined stop time. Since a chemical system remains in its current state until another reaction occurs, we can interpret the sequence $\left(\vec{x_n}, t_n\right)_{n=1}^N$ as the continuous-time trajectory $\vec{x}(t)$ of the instance.

The forms of the probability distributions in Step~2 derive in a straightforward way from the same assumption used to derive the chemical master equation. In a sense, the master equation and the SSA are two equivalent formulations of the same underlying theory.

% subsection gillespie-ssa (end)

\section{Extension to Non-Markovian Dynamics} % (fold)
\label{sub:non-markovian}

The classic SSA was designed only for explicitly Markovian chemical systems. However, one could imagine extending the algorithm to include delayed reactions. One such extension is proposed by \cite{delay-oscillations}. The modified algorithm, upon selecting a delayed reaction in Step~2 (see Section~\ref{sub:gillespie-ssa}), schedules the reaction to fire at a later time (that is, Step~3 is postponed by a time $\tau_a$).

In this work, a method that more closely follows the analytical modeling of delayed reactions via Equation~\eqref{eq:master-eqn-delay} is chosen. The SSA is modified in the propensity calculation of Step~1: To calculate the propensity of a reaction $R_j$ with delay $\delaytime_j$, we simply use the state from $\delaytime_j$ time units earlier\footnote{If $t - \tau_j$ is less than the trajectory's starting time $t_0$, we just use the initial state $\vec{x}_0$ to compute the propensity. This special case is only relevant for the first $(\max_j \tau_j)$ time units of the simulation, after which we sample the history normally.}. Thus, with a trajectory at time $t$, the propensity would be computed as $a_j = a_j\left(\vec{x}(t - \delaytime_j)\right)$. This method is consistent with the delayed-reaction formulation of the master equation, Equation~\eqref{eq:master-eqn-delay}. Note that 

One undesirable feature is introduced by including delayed reactions using either method: Since the delayed reactions lag behind the current state, it is possible for individual concentrations to go below zero, an obviously unphysical result. In the non-delayed SSA, any reaction that could decrease the concentration $x_i$ has a propensity proportional to that concentration (to some power). Therefore, when $x_i$ goes to zero, any reaction that could decrease $x_i$ has a zero propensity and is blocked from running. A delayed reaction, however, does not ``notice'' zero concentrations in real time so it is not blocked in time to avoid decreasing concentrations from zero\footnote{Some analytical models of delayed systems have this problem owing to the difficulty of analytically constraining concentrations to be nonnegative. The result is often nonsensical behavior, such as oscillations with exponentially increasing amplitude.}.

Luckily, it is much easier to impose the constraint $x_i \geq 0\; \forall i$ within the SSA than in analytical models. The method used in this work is to manually block the offending reactions: In Step~2 of the SSA, if a reaction is selected that would make any individual concentration go negative, the reaction is discarded and another is selected. This procedure amounts to manually setting that reaction's propensity to zero.

% subsection non-markovian (end)

\section{Weighted-Ensemble Resampling} % (fold)
\label{sub:we-resampling-intro}

The most straightforward way to obtain a probability distribution using the SSA is to run an ensemble of trajectories in parallel, then estimate the probability density $P(\vec{x}, t | \vec{x}_0, t_0)$ from those samples\footnote{If the probability distribution is known to be steady-state, i.e. independent of time, then one can simply average a single trajectory over a long period of time to obtain $P(\vec{x})$. However, in systems where time independence is not known \textit{a priori}, the ensemble method must be used.}.

The main issue with this method is that it generally samples state space unevenly. Trajectories in the ensemble tend to congregate, by construction, in the most probable regions of state space while less probable regions (the ``tails'' of the distribution) are more rarely visited. The result is that the accuracy of the sample of the probability distribution obtained using this simple ensemble increases with the probability density at the same location. This effect makes this method extremely inefficient if one is most interested in the least probable regions, as is the case in many problems in chemistry and biology (e.g. computing transition rates). In many cases, one would need to use a prohibitively large number of trajectories to adequately sample the improbable transition regions.

To overcome this problem, we can apply a method known (in the context of molecular dynamics and stochastic simulation) as the \defkeywd{weighted ensemble} (WE) method~\cite{we-orig}, a type of importance sampling. Its straightforward implementation, statistically exact properties, and its generality and applicability to high-dimensional concentration spaces~\cite{we-exact}\cite{we-chemkin} make it ideal for this work.

The basic strategy of the WE method is to periodically redistribute the samples in a way that does not change our estimate of the probability distribution $P(\vec{x}, t | \vec{x}_0, t_0)$. This is achieved by assigning each trajectory a \defkeywd{weight} $w_k$ such that the sum of all weights is always 1. In effect, the method biases the underlying distribution so that rare events are sampled as frequently as the common ones, while keeping track of weights in order to obtain a sample of the original, \emph{unbiased} probability distribution.

The WE method has been applied to stochastic chemical kinetics before. The implementation used in this work generally follows the version presented in \cite{we-chemkin}. The overall procedure is as follows: First, choose an initial ensemble of $P$ trajectories. Then, repeat the following steps as many times as desired:
\begin{enumerate}
    \item Perform a resampling step to equalize the distribution of trajectories over state space.
    \item Run all the trajectories independently for a time $\tau_p$.
\end{enumerate}
Step~1 is usually referred to as the \defkeywd{resampling step} and Step~2 as the \defkeywd{dynamics step}. In this case, running a trajectory means applying the SSA to evolve the trajectory forward in time. However, the generality of the WE method permits the use of other time evolution algorithms for the second step. In the more commonly encountered applications of the WE method, a molecular dynamics algorithm is used to evolve independent trajectories through phase space.

The resampling step is implemented by partitioning the state space into discrete \defkeywd{bins}. To equalize the distribution of trajectories over state space, we can use two basic operations: To increase the number of trajectories in a bin, we can replicate a trajectory $T_a$ to obtain $M$ copies. Each of the copies receives a weight $w_a/M$; in this way, the total weight within a bin is conserved. To decrease the number of trajectories,  we can a delete a trajectory $T_b$ (typically the one with the lowest weight) and assign its weight to another trajectory $T_c$ in the same bin, i.e. $T_c$ receives a new weight $w_b + w_c$. Again, the operation conserves the total weight within each bin. This conservation of weight is the key feature that ensures the statistical correctness of the resampling method.

In this work, a uniform binning strategy is adopted, i.e. the bin boundaries are spaced uniformly over some range $[x_{i,\text{min}}, x_{i,\text{max}}]$ for each coordinate $x_i$. Coordinates outside this range are grouped into the closest in-bounds bin. These bins are statically defined, so the estimate of the probability distribution is taken to be a histogram over the bins. This is by no means the only strategy possible; \cite{we-exact} gives more examples of binning strategies to show how general the weighted-ensemble method is.

% subsection we-resampling-intro (end)

\section{Weighted-Ensemble SSA} % (fold)
\label{sub:wessa}

Some important conceptual issues arise when applying the weighted-ensemble method to chemical-kinetics trajectories evolved using the SSA. The WE method is only guaranteed to be statistically exact if the resampling step occurs at the same time for all the trajectories in the ensemble. However, the SSA chooses non-uniform timesteps, so it would seem impossible to ``pause'' all the trajectories at the same time $t_\text{sync}$. 

Figure~\ref{fig:wessa-pause} illustrates the problem. The SSA only generates a discrete set of steps $(\vec{x}_n, t_n)$. Thus, the closest one can to come to pausing a trajectory at $t_\text{sync}$ is to stop each trajectory once it chooses a reaction time that is after $t_\text{sync}$, but before the trajectory actually updates its state. However, this procedure results in an ensemble of trajectories each effectively at different times, which means that it may not be correct to apply resampling to this ensemble.

\begin{figure}[ht]
    \centering
    \def\svgwidth{0.65\textwidth}
    \input{figures/wessa-timestep.pdf_tex}
    \caption{The SSA takes non-uniform timesteps, so it is not possible to pause each trajectory exactly at time $t_\text{sync}$. Instead, one must pause it at the last reaction before $t_\text{sync}$, resulting in an ensemble of trajectories each at different times.}
    \label{fig:wessa-pause}
\end{figure}

In the case of purely Markovian (non-delayed) dynamics, it is in fact perfectly valid to apply WE resampling to the set of paused trajectories described above. The reason is that the future probability distribution for a Markovian trajectory depends, by definition, only on its current state. This means that the distribution of waiting times until the next reaction on a trajectory is the memoryless exponential distribution. If a trajectory determines its next reaction to occur after the time $t_\text{sync}$, it is thus possible to update the trajectory's time to $t_\text{sync}$ and sample the next reaction from there without biasing the overall measured probability distribution.

Therefore, it is valid to pause a Markovian trajectory at an arbitrary time $t_\text{sync}$. This is statistically equivalent to leaving the trajectory at the state of its last reaction before $t_\text{sync}$. One can thus run WE resampling on trajectories paused this way without biasing the estimate of the underlying probability distribution.

% subsection wessa (end)

\section{Weighted Ensemble With Delays} % (fold)
\label{sub:we-delays}

In systems with non-Markovian dynamics, i.e. delayed reactions, a different resolution is necessary in order to be able to apply the weighted-ensemble method. Since the propensities for delayed reactions depend on a trajectory's history, they can change in between reactions. The distribution of waiting times for a given reaction is thus no longer memoryless.

To circumvent this problem, this work uses the following implementation of trajectory pausing: If a trajectory is to be paused at a time $t$, it executes the SSA until a reaction is generated that would fire later than $t$. Instead of executing that reaction, the trajectory saves it and becomes paused. Upon resuming, the trajectory recalls and executes the saved reaction and continues the SSA from the time of that reaction.

However, it is still an open question whether it is correct to pause an SSA trajectory in between reactions. The more fundamental question is whether it is valid to interpret the discrete sequence $(\vec{x}_n, t_n)$ generated by the SSA as a continuous-time trajectory. This question is not easy to address on a mathematically rigorous level. However, numerical evidence from systems with analytical descriptions indicates that WE resampling does not bias the numerical estimate of the probability distribution. This conclusion holds on systems that include delayed reactions.

% subsection we-delays (end)

% section Methodology (end)

\chapter{Results} % (fold)
\label{chp:results}

\section{Simple Production-Degradation System} % (fold)
\label{sec:simple-pd}

Accurate analytical distributions exist only for the simplest model systems. One such system is the production-degradation system, consisting of two reactions with the equations:
\begin{align}
    \begin{gathered}
        \varnothing \xrightarrow{A} X \\
        X \xrightarrow {B} \varnothing
    \end{gathered}
    \label{eq:prod-deg-rxn}
\end{align}
The parameters $A$ and $B$ are the propensity constants for each reaction. The parameter $A$ sets the scale of the system; it is often written as the product $\Omega A$ of the system's volume and a parameter of order one. Here, for simplicity, the system volume is absorbed into the constant $A$. Using this notation, the parameter $A$ plays a similar role to the volume $\Omega$.

%TODO Make distinction between population and concentration more clear
The deterministic RRE method predicts an equilibrium population of $n^\star = \frac{A}{B}$. In the Langevin limit (large $\Omega$, continuous concentration $x$), the steady-state probability distribution is predicted to be a Gaussian with standard deviation also equal to $\frac{A}{B}$. Explicitly,
\begin{equation}
    \Prob(n) = \sqrt{\frac{B}{2\pi A}}\exp\left( -\frac{B}{2 A} \left( n - \frac{A}{B} \right)^2  \right)
    \label{eq:prod-deg-dist}
\end{equation}

Alternatively, since the fluctuations in the population are expected to scale as $\sqrt{A}$, it makes sense to define the normalized fluctuation $\xi = A^{-1/2}(n - n^\star)$. In terms of $\xi$, the steady-state probability distribution is then
\begin{equation}
    \Prob(\xi) = \sqrt{\frac{B}{2\pi}} \exp(-B \xi^2)
    \label{eq:pd-dist-xi}
\end{equation}

\subsection{Weighted-Ensemble Verification} % (fold)
\label{sub:pd-we-verif}

\begin{figure}[t]
    % The "big" version
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-cmp.pgf}
        %\end{center}
        %\label{sfg:pdwe1-comp}
    %\end{subfigure}
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-devs.pgf}
        %\end{center}
        %\label{sfg:pdwe1-chi}
    %\end{subfigure}
    % More compact one-liner version
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a400-cmp-small.pgf}
            \end{center}
            \label{sfg:pdwe4-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a400-devs-small.pgf}
            \end{center}
            \label{sfg:pdwe4-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of the weighted-ensemble method with a non-resampled ensemble on the simple production-degradation system with $A=400$ and $B=3$. The probability distributions were taken after $T=60$ (arbitrary) time units. Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:pdwe4}
\end{figure}

The system was simulated using the weighted-ensemble method with individual trajectories evolved using the unmodified SSA. The ensemble was evolved in a sequence of dynamics steps of duration $\tau_p$ up to a total time $T$. At the end of the run, the probability distribution was estimated as the sum of weights within each bin. As described in Section~\ref{sub:we-resampling-intro}, the division of the state space into a number $N_b$ of bins was done uniformly over the coordinate $n$. The boundaries of the bins were linearly spaced between values $n_\text{min}$ and $n_\text{max}$. Any trajectories with states outside this range were placed in the nearest in-bounds bin. The initial trajectories used to seed the WE method were assigned random initial states chosen from the uniform distribution on $[n_\text{min}, n_\text{max}]$.

\begin{figure}[t]
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \includegraphics{../results/thesis/pdist-res-contrib.png}
            \end{center}
            \caption{}
            \label{sfg:pdist-contrib}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \includegraphics{../results/thesis/pdist-res-bincts.png}
            \end{center}
            \caption{}
            \label{sfg:pdist-bincts}
        \end{subfigure}
    }
    \caption{Data from the weighted-ensemble simulation of the simple production-degradation system in Figure~\ref{fig:pdwe4}. Left: Probability distributions obtained from the 40 independent ensembles that were run to estimate variance; each row is an independent distribution. Right: Number of trajectories in each bin at the end of the total run time, showing no empty bins; note that no resampling step was done after the final dynamics step.}
    \label{fig:pdwe4-data}
\end{figure}

Resampling was done at the beginning of each step to ensure each bin had the target number of trajectories $N_T$. Note that it is not possible to increase the number of trajectories in empty bins. By resampling often enough (choosing $\tau_p$ sufficiently small) and using a large enough target number $N_T$, we can be reasonably certain that no bins will become empty within the concentration range in which we are interested. Unless otherwise noted, the parameters used were $\tau_p = 0.01$ and $N_T = 10$; each ensemble was inspected at the end of the run to make sure all relevant bins were populated (see Figure~\ref{sfg:pdist-bincts} for an example). The total time $T$ was chosen so that the samples would be equilibriated to the steady-state distribution by then. This equilibriation time was determined by inspecting the sequence of distributions generated by a sample run of the WE simulation.

For comparison, a similar simulation was done with the resampling turned off at each step; this procedure is equivalent to running all the initial trajectories in parallel up until the time $T$. The number of trajectories used was equal to the number of bins multiplied by the target number $N_T$. The probability distribution was estimated simply as the number of trajectories within each bin (normalized by the total number of trajectories). 

A comparison between the resampled and non-resampled distributions with a reasonably large system volume is shown in Figure~\ref{fig:pdwe4}. 

\begin{figure}[t]
    % The "big" version
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-cmp.pgf}
        %\end{center}
        %\label{sfg:pdwe1-comp}
    %\end{subfigure}
    %\begin{subfigure}{\textwidth}
        %\begin{center}
            %\input{../results/thesis/pdwe-a400-devs.pgf}
        %\end{center}
        %\label{sfg:pdwe1-chi}
    %\end{subfigure}
    % More compact one-liner version
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a100-cmp-small.pgf}
            \end{center}
            \label{sfg:pdwe1-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/pdwe-a100-devs-small.pgf}
            \end{center}
            \label{sfg:pdwe1-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of weighted and non-resampled ensembles against the analytical distribution for the simple production-degradation system with $A=100$ and $B=3$ ($T=60$, $\tau_p = 0.02$), showing discreteness effects. The discrete minimum $n=0$ corresponds to $\xi \approx -3.3$. Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:pdwe1}
\end{figure}

In order to compare the two distributions in a statistically meaningful way, it is useful to estimate the uncertainty of the value of any given bin. A na\"{i}ve estimate can be obtained by running many separate ensembles with the same parameters as illustrated in Figure~\ref{sfg:pdist-contrib}, then taking the standard error $s_i$ of the values for a given bin $i$ across all the resulting distributions (this corresponds to taking the standard deviation of each column in Figure~\ref{sfg:pdist-contrib}). Reported here is the mean value of each bin across $N_\text{ens}$ independent ensembles, with the standard error on that mean estimated by $\frac{s_i}{\sqrt{N_\text{ens}}}$. The number of ensembles used was $N_\text{ens} = 40$ for all the results presented here.

\begin{figure}[ht]
    \begin{center}
        \input{../results/thesis/pdwe-a400-varcloseup.pgf}
    \end{center}
    \caption{Close-up of the probability distribution estimates of Figure~\ref{fig:pdwe4}, showing the variances in a low-probability area of the concentration space.}
    \label{fig:pdwe-varcloseup}
\end{figure}

We would expect to see the WE method improving our sampling over low-probability regions of the concentration space. Intuitively, if a bin has been visited by ten trajectories, we can be much more certain about the reported value for that bin than if we only found one trajectory there. This improved sampling should manifest itself in a reduced variance in the values of the bins at the tails of the distribution. As Figure~\ref{fig:pdwe-varcloseup} shows, though, the na\"ive estimates of the bin variances are inconsistent with this expectation. Estimated variances for the resampled and non-resampled distributions are approximately the same even in the tail of the distribution. This characteristic is reflected in all the weighted-ensemble datasets presented in this work.

This inconsistency of the variance estimates with expectations could indicate one of two things. First, the na\"ive estimate of bin variance could be failing to capture the improvement in sampling that the WE method offers. This seems unlikely, however, as the variances were estimated in a statistically correct manner. The second possible effect is that the target number $N_T$ of trajectories per bin is not large enough to afford a significant improvement in the variance of low-probability bins. Intuitively, however, we would expect a variance reduction even with the numbers used here ($N_T = 10$ trajectories versus one or two in some non-resampled bins). Perhaps it would be possible to get a better estimate of the variance of a bin by taking into account the number of trajectories, i.e. the number of samples, that were used to find its value.

In any case, neither of the comparisons here shows a \emph{systematic} difference between the resampled and non-resampled distributions. Therefore, we can conclude that WE resampling does not bias our estimate of the probability distribution for the simple production-degradation system.

This conclusion is exactly what one would expect, as this system has no delayed reactions and is therefore Markovian. As discussed in Section~\ref{sub:wessa}, WE resampling is guaranteed to be correct for Markovian systems. For the purposes of verifying the WE method, it is more interesting to study systems that incorporate delayed reactions.

% subsection pd-we-verif (end)

% subsection simple-pd (end)

\section{Delayed-Degradation System} % (fold)
\label{sec:delayed-deg}

One of the simplest possible systems that includes delayed reactions is the delayed-degradation model, defined by the reactions:
\begin{align}
    \begin{gathered}
        \varnothing \xrightarrow{A} X \\
        X \xrightarrow {B} \varnothing \\
        X \xRightarrow[(\tau)]{C} \varnothing
    \end{gathered}
    \label{eq:delayed-deg-rxns}
\end{align}
This is simply the production-degradation system with an additional reaction added; the notation for the third reaction means it is a delayed reaction with propensity constant $C$ (analogous to $B$) and delay $\tau$. As in the simple production-degradation model the parameter $A$ sets the scale of the system, absorbing the system volume $\Omega$.

The additional delayed reaction has a profound effect on the overall qualitative behavior of this system. As the value of $C$ is varied, the system undergoes a \defkeywd{bifurcation}, i.e. the system goes into a different qualitative mode of behavior. For large enough values of $C$, the concentration oscillates periodically between two values (the ``oscillatory mode''). However, for smaller values of $C$ the system behaves much like the simple production-degradation system, fluctuating about a stable mean value (the ``stable mode''). An analytical form for the probability distribution is only available for the stable mode. In the limit of short delay $\tau$, the distribution is \cite{delayed-deg-notes}
\begin{equation}
    P(\xi) = \sqrt{\frac{B + C}{2\pi(1 + C\tau)}}\exp\left( -\frac{B + C}{2(1 + C\tau)} \xi^2 \right)
    \label{eq:dd-analytic-dist}
\end{equation}
where $\xi$ is defined as in Section~\ref{sec:simple-pd}, only here the steady-state mean population is $n^\star = \frac{A}{B + C}$. The effect of the delay is to broaden the distribution compared to the simple production-degradation system.

\subsection{Weighted-Ensemble Verification} % (fold)
\label{sub:dd-we-verif}

The procedure used to assess the effect of resampling on the numerical probability distribution for this system is the same as the one of Section~\ref{sub:pd-we-verif}, with a few modifications. The first modification accounts for the fact that the delay introduces an effective phase to each trajectory (the effect is more obvious in the oscillatory mode). To ensure that the probability distributions were sampled uniformly over this phase, the starting times of the seed trajectories were randomized relative to each other. Specifically, the initial time of each trajectory, relative to the ensemble time, was drawn from a uniform distribution on $[0, \tau]$.

\begin{figure}[tbp]
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t0,1-a800-cmp-small.pgf}
            \end{center}
            \label{sfg:ddwe0-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t0,1-a800-devs-small.pgf}
            \end{center}
            \label{sfg:ddwe0-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of weighted and non-resampled ensembles against the analytical distribution for the delayed-degradation system with $A=800$, $B=3$, $C=1$, and $\tau=0.1$ ($T=3$). Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:ddwe0}
\end{figure}

Second, the total time $T$ that each ensemble was run had to be chosen to avoid any effects due to the initial phase randomization. The value $T = 3\tau$ was deemed a safe choice for this purpose, as trajectories sample their histories only as far as $\tau$ time units earlier. By time $3\tau$, all trajectories would have been sampling active histories (that is, not sampling earlier than their starting times) for at least $\tau$ time units.

\begin{figure}[tbp]
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t1-a800-cmp-small.pgf}
            \end{center}
            \label{sfg:ddwe1-comp}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t1-a800-devs-small.pgf}
            \end{center}
            \label{sfg:ddwe1-chi}
        \end{subfigure}
    }
    \caption{Left: comparison of weighted and non-resampled ensembles against the analytical distribution for the delayed-degradation system with $A=800$, $B=3$, $C=1$, and $\tau=1$ ($T=3$), showing deviation from the analytical variance derived for small $\tau$. Right: differences from the analytical distribution, normalized by each estimate's standard deviation.}
    \label{fig:ddwe1}
\end{figure}

Figure~\ref{fig:ddwe0} shows a comparison between the simulation results, both with and without resampling, and the analytical distribution \eqref{eq:dd-analytic-dist}. As expected, the simulations agree well with the analytical distribution for a relatively short delay of $\tau = 0.1$. The agreement with the analytical distribution begins to break down for larger values of $\tau$; see Figure~\ref{fig:ddwe1}. However, as Figure~\ref{fig:ddwe-resdevs} shows, the distributions obtained with resampling and without using resampling still agree well with each other, with no obvious systematic differences. 

\begin{figure}[tbp]
    \makebox[\linewidth][c]{
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t0,1-a800-resdevs.pgf}
            \end{center}
            \label{sfg:ddwe-resdevs0}
        \end{subfigure}
        \begin{subfigure}{3in}
            \begin{center}
                \input{../results/thesis/ddwe-t1-a800-resdevs.pgf}
            \end{center}
            \label{sfg:ddwe-resdevs1}
        \end{subfigure}
    }
    \caption{Differences between the distributions obtained with and without resampling, normalized by the two-parameter standard deviations $\sqrt{\sigma_\text{res}^2 + \sigma_\text{nonres}^2}$ for each bin. Parameters: $A=800$, $B=3$, $C=1$. Left: $\tau = 0.1$. Right: $\tau = 1$.}
    \label{fig:ddwe-resdevs}
\end{figure}

Based on this data, we can tentatively conclude that weighted-ensemble resampling does not bias our estimate of the steady-state probability distribution of the delayed-degradation system. This is a stronger conclusion than the one for the simple production-degradation system. This data provides strong evidence that, even in the presence of delayed reactions, the WE resampling method does not have a biasing effect. It remains to be seen whether this conclusion holds in general for more complex delayed systems, although this data does provide good reason to be optimistic.

% subsection dd-we-verif (end)

\subsection{Delayed Joint Probability Distribution} % (fold)
\label{sub:delayed-joint-dist}
The chemical master equation (Equation \eqref{eq:master-eqn-delay}) written for the delayed-degradation system takes the form:
\begin{multline}
    \frac{\partial}{\partial t} \Prob(n, t) = A(\hat{E}^{-1} - 1) \Prob(n, t) + B n(\hat{E} - 1) \Prob(n, t) + \\
    \sum_{m=0}^{\infty} H(n) C m (\hat{E} - 1) \Prob_2(n, t; m, t - \tau)
    \label{eq:master-eqn-dd}
\end{multline}
with $\hat{E}$ representing the unit shift operator acting only on $n$, i.e.
\[
    \hat{E}{f(n,t;} {m,t-\tau)} = {f(n+1,t;} {m,t-\tau)}.
\]

The equation suffers from a closure problem due to the non-Markovian properties of this system. The question is how to model the joint probability distribution $P_2(m,t; n, t - \tau)$. Writing an equation for the time evolution of this distribution only results in a dependence on higher-order distributions. Analytical models of the delayed-degradation system therefore need to model this joint distribution in some other way.

To cast Equation~\eqref{eq:master-eqn-dd} in a slightly more convenient form, we can eliminate the sum over $m$ by replacing the joint probability distribution with the \defkeywd{conditional average}:
\begin{equation}
    \langle m, t - \tau | n, t \rangle = \frac{\sum_m m P_2(n, t; m, t - \tau)}{\sum_m P_2(n, t; m, t - \tau) }
    \label{eq:cond-avg-def}
\end{equation}
The master equation then takes the form \cite{delayed-deg-notes}:
\begin{multline}
    \frac{\partial}{\partial t} \Prob(n, t) = A(\hat{E}^{-1} - 1) \Prob(n, t) + B n(\hat{E} - 1) \Prob(n, t) + \\
    C (\hat{E} - 1) (\langle m, t - \tau | n, t \rangle \Prob(n, t))
    \label{eq:master-eqn-dd-ca}
\end{multline}

One method for modeling the delayed joint distribution is used in \cite{delay-oscillations}, a type of mean-field approximation where events at time $t$ are assumed to be decorrelated from events at time $t - \tau$. This assumption implies $\langle m, t - \tau | n, t \rangle = \langle m \rangle$ -- in other words, that the conditional average of $m(t - \tau)$ is a constant that does not depend on $n(t)$. The authors of \cite{delay-oscillations} note that in order for this assumption to hold, the coupling between events at time $t$ and $t - \tau$, which is controlled by the value of $C$, should be relatively small. The approximation could very well break down close to the bifurcation.

One way to evaluate the conditions under which this assumption holds is to use stochastic simulation, which does not suffer from the closure problem encountered in analytical models. It is possible to compute the delayed joint distribution based on a single trajectory generated by the SSA. Interpreting the sequence $(n_i, t_i)$ as a continuous-time trajectory $n(t)$, the probability $P_2(p, t; q, t - \tau)$ is proportional to the amount of time the system spends in the state $n(t) = p; n(t - \tau) = q$. More formally,
\begin{equation}
    P_2(p, t; q, t - \tau) = \frac{1}{t_1 - t_0} \int_{t_0}^{t_1} \delta_{p, n(t)} \delta_{q, n(t - \tau)} \dee t
    \label{eq:joint-pdist-calc}
\end{equation}
where $t_0$ is at least $\tau$ time units later than the trajectory's starting time; $\delta_{i,j}$ is the Kronecker delta symbol. The conditional average is then straightforward to calculate from Equation~\eqref{eq:cond-avg-def}.

\begin{figure}[tb]
    \makebox[\linewidth][c]{
        \includegraphics{../results/thesis/ddjd-a100-b3-t20-csweep.png}
    }
    \caption{Behavior of the delayed joint distribution as the parameter $C$ is varied. The parameters $A=100$, $B=3$, and $\tau=20$ were kept fixed; each distribution was computed from $T = t_1 - t_0 = 800$ time units of a single SSA trajectory with initial state $n_0 = 0$.}
    \label{fig:ddjd-sweep}
\end{figure}

Figure~\ref{fig:ddjd-sweep} shows the qualitative behavior of the delayed joint distribution as the parameter $C$ (the coupling between events at time $t$ and time $t - \tau$) is varied. For very weak coupling, the distribution is essentially an uncorrelated two-dimensional Gaussian. As the coupling is increased, however, the central peak stretches and eventually splits into two. For very strong coupling, the distribution is characterized by two completely separated peaks.

\begin{figure}[tb]
    \begin{center}
        \includegraphics{../results/thesis/ddjd-ca-a100-c2-t1.png}
    \end{center}
    \caption{Conditional average of the delayed joint distribution with $A=100$, $B=3$, $C=2$, and $\tau=1$ computed from $T=800$ time units of an SSA trajectory, showing approximate independence of the conditional average of $q$ from $p$. The flat magenta line indicates the average value of $q$ over the entire trajectory. The delayed-joint distribution is provided in the background for context; the conditional average is simply a moment of that distribution.}
    \label{fig:ddjd-stable}
\end{figure}

We can evaluate the extent to which the mean-field approximation applies to the system by computing the conditional average of the SSA-derived distribution. The conditional average $\langle q, t - \tau | p, t \rangle$ for a reasonably small value of $C$ is shown in Figure~\ref{fig:ddjd-stable}. The mean-field approximation implies a conditional average independent of $p$, or a horizontal line on the plot in Figure~\ref{fig:ddjd-stable}. The approximation appears reasonably good for the value of $C$ shown there; we could expect better agreement with a smaller value of $C$.

Closer to the bifurcation, however, this approximation completely breaks down. Figure~\ref{fig:ddjd-bif} shows an example for a value of $C$ just below the stochastic bifurcation point. The highly correlated nature of the delayed joint distribution in this regime evidently requires a different description from the mean-field approximation.

\begin{figure}[tb]
    \makebox[\linewidth][c]{
    \begin{subfigure}{3in}
        \includegraphics{../results/thesis/ddjd-ca-a400-c5-t1.png}
    \end{subfigure}
    \begin{subfigure}{3in}
        \includegraphics{../results/thesis/ddjd-ca-a100-c5,8-t1.png}
    \end{subfigure}
    }
    \caption{Conditional average of the delayed joint distribution near the bifurcation, showing a breakdown of the mean-field approximation and saturation for large values of $p$. Left: $A=400$, $B=4.5$, $C=5$, and $\tau=1$. Right: $A=100$, $B=4.5$, $C=5.75$, and $\tau=1$. The straight blue line indicates the prediction of Equation~\eqref{eq:cond-avg-linear}. Each distribution was computed from $T=800$ time units of an SSA trajectory.}
    \label{fig:ddjd-bif}
\end{figure}

Another model for the conditional average can be made in the Langevin limit and near the stochastic bifurcation point. In terms of the normalized fluctuation $\xi = \frac{n - n^\star}{\sqrt{A}}$ defined in Section~\ref{sub:dd-we-verif}, the conditional average in ths limit is the linear function \cite{delayed-deg-notes}
\begin{equation}
    \langle \xi_\tau, t - \tau | \xi, t \rangle = \left( \frac{\omega \cos(\omega \tau) - B \sin(\omega \tau)}{\omega + C \sin(\omega \tau)}\right)\xi 
    \label{eq:cond-avg-linear}
\end{equation}
with the Hopf frequency $\omega = \sqrt{C^2 - B^2}$, so this equation is only valid for $C > B$. The derivation of this formula also assumes the system is below the stochastic bifurcation point\footnote{The stochastic bifurcation point is in general different from the bifurcation point that can be derived from a deterministic description of this system. Stochastic effects shift the bifurcation, opening up a ``window'' in which Equation~\eqref{eq:cond-avg-linear} can be applied.}, i.e. that the delayed-joint distribution is unimodal.

\begin{figure}[tb]
    \begin{center}
        \includegraphics{../results/thesis/ddjd-ca-a100-c5-t20.png}
    \end{center}
    \caption{Conditional average of the delayed-joint distribution with $A=100$, $B=3$, $C=5$, and $\tau=20$ computed from $T=800$ time units of an SSA trajectory, showing saturation to a constant value. The flat blue line indicates the average value of $q$ only over the lower peak.}
    \label{fig:ddjd-osc-sat}
\end{figure}

This formula agrees well with the computed conditional average within a certain range of $p$, as Figure~\ref{fig:ddjd-bif} shows. However, an interesting effect occurs for large values of $p$, where the computed conditional average deviates from Equation~\ref{eq:cond-avg-linear}. In the complete stochastic description modeled by the SSA the number $n$ cannot go below zero. This means that the conditional average must eventually flatten out, or saturate, for large values of $p$. The Langevin limit does not generally impose this constraint, so the conditional average as predicted by Equation~\eqref{eq:cond-avg-linear} does go below zero above some value of $p$.

The saturation effect is most important in small systems because they have lower average concentrations and spend more of their time close to $n=0$. This observation is consistent with the fact that the Langevin limit, which in this case fails to account for saturation, becomes more accurate as the system volume is increased.

Interestingly enough, the saturation effect appears to partially restore the mean-field approximation in the oscillatory mode. As Figure~\ref{fig:ddjd-osc-sat} shows, the conditional average for an oscillating system becomes essentially constant for large enough values of $p$. Thus, we can cautiously assume (for large enough values of $p$) that the conditional average $\langle q, t - \tau | p, t \rangle$ is independent of $p$ far enough from the bifurcation in \emph{both} the weak and strong coupling limits.

% subsection delayed-joint-dist (end)

% section delayed-deg (end)

\chapter{Conclusions and Future Work} % (fold)
\label{sec:conclusions}

The weighted-ensemble stochastic simulation algorithm is a useful tool for studying genetic networks with delays. The SSA explicitly treats the discreteness and stochasticity that become significant in small systems, allowing us to describe them in a statistically exact manner. In addition, it is straightforward to extend the SSA to include delayed reactions. The WE resampling method ensures that we sample the system's state space evenly, enabling us to simulate realistic, complex genetic networks efficiently. Results from this work indicate that the resampling maintains statistical accuracy, i.e. that we can obtain an unbiased estimate of the measured distribution, even in systems containing delayed reactions.

The results of Section~\ref{sub:delayed-joint-dist} illustrate another important application of the SSA as a statistically exact numerical simulation. We can use it to assess approximations or assumptions that analytical models make to study a system's behavior. We can also use it as a stand-alone tool to explore how a system behaves. The example discussed in Section~\ref{sub:delayed-joint-dist} was the delayed-degradation system, where we saw that the behavior of the system is decorrelated on the timescale $\tau$ as long as the delayed reaction fires relatively rarely. This approximation breaks down near the bifurcation point. However, we saw that the approximation is restored in a sense on the other side of the bifurcation. This is a conclusion that is not at all obvious from an analytical or intuitive description of the system.

% Future Work
The next step in this project is to model systems that are also realizable experimentally. A comparison between numerical predictions and experimental results would serve as an excellent check on the validity of the assumptions underlying the application of the SSA to genetic networks.

\section{Modeling Crowded Environments} % (fold)
\label{sub:diffusion-crowded}

The most important assumption underlying the SSA and the chemical master equation is that the chemical system is well-stirred (homogeneous) and behaves like an ideal gas \cite{langevin-limit}. Both of these assumptions often break down, to some extent, in living cells. First, cells are highly inhomogeneous environments, with various internal membranes (especially, in eukaryotic cells, the nuclear membrane) hindering the free movement of molecules throughout the cell. Second, the high concentrations of bulky molecules such as proteins would also be expected to slow down molecular diffusion~\cite{art-cells-crowding}.

There are several possible ways to model these deviations from the ideal-gas assumption. One way would be to add a phenomenological retardation factor that decreases each reaction's propensity in the presence of high concentrations of other molecules. Another way would be to run the SSA on a spatial grid of cells coupled by a diffusion law.

Although it may seem improper to model a complex cellular environment as a simple ideal gas, this method still seems to be the dominant one for modeling genetic regulatory networks. Comparison with experimental data should be the most useful guide in determining whether the ideal-gas assumption is in fact justified or whether one of the above strategies needs to be applied to realistically model genetic networks in cells.

% subsection diffusion-crowded (end)

% section conclusions (end)

% chapter results (end)

\appendix

\chapter{Technical Notes} % (fold)
\label{sec:tech-notes}

I implemented all computational algorithms used in this work in Python version 3.4.0 (\url{http://python.org}) using the packages SciPy 0.13.3 and NumPy 1.8.1 (\url{http://scipy.org}). I used the IPython interactive scientific computing system~\cite{PER-GRA:2007} to help develop and test my code, along with the package Matplotlib (\url{http://matplotlib.org}) to explore and visualize my results. Pseudorandom numbers for sampling from probability distributions in my implementation of the SSA (Section~\ref{sub:gillespie-ssa}) came from NumPy, which uses the Mersenne twister algorithm.

All the code written for this project, including Python modules and IPython notebooks, is publicly available at \url{https://github.com/max-veit/wessa-delay}. The code is versioned in a Git repository. The version of the code used to obtain the results in this thesis is recorded under the commit with the SHA1 hash \texttt{7b259428f475041697e7ed8ddca7719518d83dbd} and tagged with the name \texttt{thesis-final}. The code is also available via the supporting information accompanying this document in the University Digital Conservancy.

% If publishing to GitHub, be sure to mention tag and hash: 7b259428f475041697e7ed8ddca7719518d83dbd

% section tech-notes (end)

\end{doublespacing}

\bibliographystyle{apsrev}
\bibliography{citations.bib}

\end{document}

